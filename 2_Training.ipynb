{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Nanodegree\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will train your CNN-RNN model.  \n",
    "\n",
    "You are welcome and encouraged to try out many different architectures and hyperparameters when searching for a good model.\n",
    "\n",
    "This does have the potential to make the project quite messy!  Before submitting your project, make sure that you clean up:\n",
    "- the code you write in this notebook.  The notebook should describe how to train a single CNN-RNN architecture, corresponding to your final choice of hyperparameters.  You should structure the notebook so that the reviewer can replicate your results by running the code in this notebook.  \n",
    "- the output of the code cell in **Step 2**.  The output should show the output obtained when training the model from scratch.\n",
    "\n",
    "This notebook **will be graded**.  \n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Training Setup\n",
    "- [Step 2](#step2): Train your Model\n",
    "- [Step 3](#step3): (Optional) Validate your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup\n",
    "\n",
    "In this step of the notebook, you will customize the training of your CNN-RNN model by specifying hyperparameters and setting other options that are important to the training procedure.  The values you set now will be used when training your model in **Step 2** below.\n",
    "\n",
    "You should only amend blocks of code that are preceded by a `TODO` statement.  **Any code blocks that are not preceded by a `TODO` statement should not be modified**.\n",
    "\n",
    "### Task #1\n",
    "\n",
    "Begin by setting the following variables:\n",
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. \n",
    "- `vocab_threshold` - the minimum word count threshold.  Note that a larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary.  \n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from file. \n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  \n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder.  \n",
    "- `num_epochs` - the number of epochs to train the model.  We recommend that you set `num_epochs=3`, but feel free to increase or decrease this number as you wish.  [This paper](https://arxiv.org/pdf/1502.03044.pdf) trained a captioning model on a single state-of-the-art GPU for 3 days, but you'll soon see that you can get reasonable results in a matter of a few hours!  (_But of course, if you want your model to compete with current research, you will have to train for much longer._)\n",
    "- `save_every` - determines how often to save the model weights.  We recommend that you set `save_every=1`, to save the model weights after each epoch.  This way, after the `i`th epoch, the encoder and decoder weights will be saved in the `models/` folder as `encoder-i.pkl` and `decoder-i.pkl`, respectively.\n",
    "- `print_every` - determines how often to print the batch loss to the Jupyter notebook while training.  Note that you **will not** observe a monotonic decrease in the loss function while training - this is perfectly fine and completely expected!  You are encouraged to keep this at its default value of `100` to avoid clogging the notebook, but feel free to change it.\n",
    "- `log_file` - the name of the text file containing - for every step - how the loss and perplexity evolved during training.\n",
    "\n",
    "If you're not sure where to begin to set some of the values above, you can peruse [this paper](https://arxiv.org/pdf/1502.03044.pdf) and [this paper](https://arxiv.org/pdf/1411.4555.pdf) for useful guidance!  **To avoid spending too long on this notebook**, you are encouraged to consult these suggested research papers to obtain a strong initial guess for which hyperparameters are likely to work best.  Then, train a single model, and proceed to the next notebook (**3_Inference.ipynb**).  If you are unhappy with your performance, you can return to this notebook to tweak the hyperparameters (and/or the architecture in **model.py**) and re-train your model.\n",
    "\n",
    "### Question 1\n",
    "\n",
    "**Question:** Describe your CNN-RNN architecture in detail.  With this architecture in mind, how did you select the values of the variables in Task 1?  If you consulted a research paper detailing a successful implementation of an image captioning model, please provide the reference.\n",
    "\n",
    "**Answer:** My CNN architecture uses the resnet50 architecture as recommended with a trained linear layer with batch normisation at the end. Batch normalisation was added as it showed to be helpful in the previous (facial keypoint) detection tasks. The RNN is a LSTM with 1 hidden layer and an embedding of 512 as [this paper](https://arxiv.org/pdf/1502.03044.pdf) showed to be successful with such parameters\n",
    "\n",
    "### (Optional) Task #2\n",
    "\n",
    "Note that we have provided a recommended image transform `transform_train` for pre-processing the training images, but you are welcome (and encouraged!) to modify it as you wish.  When modifying this transform, keep in mind that:\n",
    "- the images in the dataset have varying heights and widths, and \n",
    "- if using a pre-trained model, you must perform the corresponding appropriate normalization.\n",
    "\n",
    "### Question 2\n",
    "\n",
    "**Question:** How did you select the transform in `transform_train`?  If you left the transform at its provided value, why do you think that it is a good choice for your CNN architecture?\n",
    "\n",
    "**Answer:** The transform used the same transform provided as it looked to be robust and provided the nessassary changes to ensure the images are different. Also from the previous task a similar transform was performed\n",
    "\n",
    "### Task #3\n",
    "\n",
    "Next, you will specify a Python list containing the learnable parameters of the model.  For instance, if you decide to make all weights in the decoder trainable, but only want to train the weights in the embedding layer of the encoder, then you should set `params` to something like:\n",
    "```\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "```\n",
    "\n",
    "### Question 3\n",
    "\n",
    "**Question:** How did you select the trainable parameters of your architecture?  Why do you think this is a good choice?\n",
    "\n",
    "**Answer:** Trainable paramters were selected based on trial and error. Initially an embedding size of 256 was used but the results didn't seem to reduce loss after some time and produced average results. Also 1 hidden layer for the RNN was chosen as it was recommended in the paper highlighted previously.\n",
    "\n",
    "### Task #4\n",
    "\n",
    "Finally, you will select an [optimizer](http://pytorch.org/docs/master/optim.html#torch.optim.Optimizer).\n",
    "\n",
    "### Question 4\n",
    "\n",
    "**Question:** How did you select the optimizer used to train your model?\n",
    "\n",
    "**Answer:** Originally the Adam Optimizer was chosen to train the model but it seemed to stop learning after a number of steps and the loss didn't decrease when reaching this threshold. A SGD was then chosen to hopefully solve this problem and find global optima instead of local optima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=1.12s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 545/414113 [00:00<01:15, 5444.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:55<00:00, 7429.10it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "def to_var(x, volatile=False):\n",
    "    \"\"\" converts a Pytorch Tensor to a variable and moves to GPU if CUDA is available \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x, volatile=volatile)\n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "batch_size = 10           # batch size\n",
    "vocab_threshold = 5      # minimum word count threshold\n",
    "vocab_from_file = True      # if True, load existing vocab file\n",
    "embed_size = 512           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 3             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "if torch.cuda.is_available():\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) + list(encoder.bn.parameters())\n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "optimizer = torch.optim.SGD(params, lr=0.1)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Train your Model\n",
    "\n",
    "Once you have executed the code cell in **Step 1**, the training procedure below should run without issue.  \n",
    "\n",
    "It is completely fine to leave the code cell below as-is without modifications to train your model.  However, if you would like to modify the code used to train the model below, you must ensure that your changes are easily parsed by your reviewer.  In other words, make sure to provide appropriate comments to describe how your code works!  \n",
    "\n",
    "You may find it useful to load saved weights to resume training.  In that case, note the names of the files containing the encoder and decoder weights that you'd like to load (`encoder_file` and `decoder_file`).  Then you can load the weights by using the lines below:\n",
    "\n",
    "```python\n",
    "# Load pre-trained weights before resuming training.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
    "```\n",
    "\n",
    "While trying out parameters, make sure to take extensive notes and record the settings that you used in your various training runs.  In particular, you don't want to encounter a situation where you've trained a model for several hours but can't remember what settings you used :).\n",
    "\n",
    "### A Note on Tuning Hyperparameters\n",
    "\n",
    "To figure out how well your model is doing, you can look at how the training loss and perplexity evolve during training - and for the purposes of this project, you are encouraged to amend the hyperparameters based on this information.  \n",
    "\n",
    "However, this will not tell you if your model is overfitting to the training data, and, unfortunately, overfitting is a problem that is commonly encountered when training image captioning models.  \n",
    "\n",
    "For this project, you need not worry about overfitting. **This project does not have strict requirements regarding the performance of your model**, and you just need to demonstrate that your model has learned **_something_** when you generate captions on the test data.  For now, we strongly encourage you to train your model for the suggested 3 epochs without worrying about performance; then, you should immediately transition to the next notebook in the sequence (**3_Inference.ipynb**) to see how your model performs on the test data.  If your model needs to be changed, you can come back to this notebook, amend hyperparameters (if necessary), and re-train the model.\n",
    "\n",
    "That said, if you would like to go above and beyond in this project, you can read about some approaches to minimizing overfitting in section 4.3.1 of [this paper](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505636).  In the next (optional) step of this notebook, we provide some guidance for assessing the performance on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch [1/3], Step [1/41412], Loss: 9.0889, Perplexity: 8856.2324"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:16: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:58: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/41412], Loss: 6.6478, Perplexity: 771.0596\n",
      "Epoch [1/3], Step [200/41412], Loss: 5.7420, Perplexity: 311.70079\n",
      "Epoch [1/3], Step [300/41412], Loss: 4.9804, Perplexity: 145.5257\n",
      "Epoch [1/3], Step [400/41412], Loss: 5.0365, Perplexity: 153.9371\n",
      "Epoch [1/3], Step [500/41412], Loss: 4.7532, Perplexity: 115.9495\n",
      "Epoch [1/3], Step [600/41412], Loss: 3.5847, Perplexity: 36.04310\n",
      "Epoch [1/3], Step [700/41412], Loss: 3.6731, Perplexity: 39.37252\n",
      "Epoch [1/3], Step [800/41412], Loss: 3.6067, Perplexity: 36.84258\n",
      "Epoch [1/3], Step [900/41412], Loss: 3.7803, Perplexity: 43.82751\n",
      "Epoch [1/3], Step [1000/41412], Loss: 4.4711, Perplexity: 87.4491\n",
      "Epoch [1/3], Step [1100/41412], Loss: 3.9566, Perplexity: 52.28049\n",
      "Epoch [1/3], Step [1200/41412], Loss: 4.6324, Perplexity: 102.7625\n",
      "Epoch [1/3], Step [1300/41412], Loss: 4.6677, Perplexity: 106.4546\n",
      "Epoch [1/3], Step [1400/41412], Loss: 3.9046, Perplexity: 49.62948\n",
      "Epoch [1/3], Step [1500/41412], Loss: 3.2681, Perplexity: 26.26080\n",
      "Epoch [1/3], Step [1600/41412], Loss: 3.7660, Perplexity: 43.20506\n",
      "Epoch [1/3], Step [1700/41412], Loss: 4.1569, Perplexity: 63.87241\n",
      "Epoch [1/3], Step [1800/41412], Loss: 3.8472, Perplexity: 46.8603\n",
      "Epoch [1/3], Step [1900/41412], Loss: 3.0580, Perplexity: 21.28550\n",
      "Epoch [1/3], Step [2000/41412], Loss: 3.7888, Perplexity: 44.20362\n",
      "Epoch [1/3], Step [2100/41412], Loss: 3.7179, Perplexity: 41.17826\n",
      "Epoch [1/3], Step [2200/41412], Loss: 3.9235, Perplexity: 50.57599\n",
      "Epoch [1/3], Step [2300/41412], Loss: 3.8376, Perplexity: 46.41294\n",
      "Epoch [1/3], Step [2400/41412], Loss: 3.6456, Perplexity: 38.3070\n",
      "Epoch [1/3], Step [2500/41412], Loss: 3.4552, Perplexity: 31.6647\n",
      "Epoch [1/3], Step [2600/41412], Loss: 3.2564, Perplexity: 25.9559\n",
      "Epoch [1/3], Step [2700/41412], Loss: 3.7676, Perplexity: 43.27517\n",
      "Epoch [1/3], Step [2800/41412], Loss: 2.9222, Perplexity: 18.58129\n",
      "Epoch [1/3], Step [2900/41412], Loss: 3.6128, Perplexity: 37.0702\n",
      "Epoch [1/3], Step [3000/41412], Loss: 2.9138, Perplexity: 18.42743\n",
      "Epoch [1/3], Step [3100/41412], Loss: 4.4855, Perplexity: 88.7220\n",
      "Epoch [1/3], Step [3200/41412], Loss: 3.5310, Perplexity: 34.1595\n",
      "Epoch [1/3], Step [3300/41412], Loss: 3.2215, Perplexity: 25.06682\n",
      "Epoch [1/3], Step [3400/41412], Loss: 3.6231, Perplexity: 37.45382\n",
      "Epoch [1/3], Step [3500/41412], Loss: 3.3208, Perplexity: 27.6825\n",
      "Epoch [1/3], Step [3600/41412], Loss: 3.0706, Perplexity: 21.5548\n",
      "Epoch [1/3], Step [3700/41412], Loss: 3.1043, Perplexity: 22.2932\n",
      "Epoch [1/3], Step [3800/41412], Loss: 3.5143, Perplexity: 33.59161\n",
      "Epoch [1/3], Step [3900/41412], Loss: 3.6827, Perplexity: 39.7534\n",
      "Epoch [1/3], Step [4000/41412], Loss: 3.2838, Perplexity: 26.6781\n",
      "Epoch [1/3], Step [4100/41412], Loss: 3.2261, Perplexity: 25.1811\n",
      "Epoch [1/3], Step [4200/41412], Loss: 3.0719, Perplexity: 21.5839\n",
      "Epoch [1/3], Step [4300/41412], Loss: 3.0839, Perplexity: 21.84296\n",
      "Epoch [1/3], Step [4400/41412], Loss: 3.6666, Perplexity: 39.1177\n",
      "Epoch [1/3], Step [4500/41412], Loss: 3.1213, Perplexity: 22.67623\n",
      "Epoch [1/3], Step [4600/41412], Loss: 3.8372, Perplexity: 46.3972\n",
      "Epoch [1/3], Step [4700/41412], Loss: 3.0178, Perplexity: 20.4463\n",
      "Epoch [1/3], Step [4800/41412], Loss: 2.8042, Perplexity: 16.5131\n",
      "Epoch [1/3], Step [4900/41412], Loss: 2.7779, Perplexity: 16.0851\n",
      "Epoch [1/3], Step [5000/41412], Loss: 3.2181, Perplexity: 24.9796\n",
      "Epoch [1/3], Step [5100/41412], Loss: 3.4238, Perplexity: 30.6865\n",
      "Epoch [1/3], Step [5200/41412], Loss: 3.1835, Perplexity: 24.1315\n",
      "Epoch [1/3], Step [5300/41412], Loss: 3.3939, Perplexity: 29.7811\n",
      "Epoch [1/3], Step [5400/41412], Loss: 2.8840, Perplexity: 17.8865\n",
      "Epoch [1/3], Step [5500/41412], Loss: 3.1740, Perplexity: 23.9029\n",
      "Epoch [1/3], Step [5600/41412], Loss: 2.9185, Perplexity: 18.5141\n",
      "Epoch [1/3], Step [5700/41412], Loss: 3.9197, Perplexity: 50.3859\n",
      "Epoch [1/3], Step [5800/41412], Loss: 3.2359, Perplexity: 25.42985\n",
      "Epoch [1/3], Step [5900/41412], Loss: 2.9405, Perplexity: 18.9247\n",
      "Epoch [1/3], Step [6000/41412], Loss: 2.9097, Perplexity: 18.3512\n",
      "Epoch [1/3], Step [6100/41412], Loss: 3.6180, Perplexity: 37.2645\n",
      "Epoch [1/3], Step [6200/41412], Loss: 2.6487, Perplexity: 14.13630\n",
      "Epoch [1/3], Step [6300/41412], Loss: 2.7591, Perplexity: 15.7851\n",
      "Epoch [1/3], Step [6400/41412], Loss: 4.2832, Perplexity: 72.4717\n",
      "Epoch [1/3], Step [6500/41412], Loss: 2.8160, Perplexity: 16.70985\n",
      "Epoch [1/3], Step [6600/41412], Loss: 3.0403, Perplexity: 20.9120\n",
      "Epoch [1/3], Step [6700/41412], Loss: 2.7383, Perplexity: 15.46012\n",
      "Epoch [1/3], Step [6800/41412], Loss: 2.6785, Perplexity: 14.5635\n",
      "Epoch [1/3], Step [6900/41412], Loss: 2.3905, Perplexity: 10.91842\n",
      "Epoch [1/3], Step [7000/41412], Loss: 2.6437, Perplexity: 14.0647\n",
      "Epoch [1/3], Step [7100/41412], Loss: 3.1459, Perplexity: 23.2407\n",
      "Epoch [1/3], Step [7200/41412], Loss: 3.1880, Perplexity: 24.2397\n",
      "Epoch [1/3], Step [7300/41412], Loss: 3.2318, Perplexity: 25.3242\n",
      "Epoch [1/3], Step [7400/41412], Loss: 3.2461, Perplexity: 25.6887\n",
      "Epoch [1/3], Step [7500/41412], Loss: 2.8309, Perplexity: 16.9603\n",
      "Epoch [1/3], Step [7600/41412], Loss: 3.5051, Perplexity: 33.2847\n",
      "Epoch [1/3], Step [7700/41412], Loss: 3.7383, Perplexity: 42.0247\n",
      "Epoch [1/3], Step [7800/41412], Loss: 2.8036, Perplexity: 16.5042\n",
      "Epoch [1/3], Step [7900/41412], Loss: 3.5128, Perplexity: 33.5412\n",
      "Epoch [1/3], Step [8000/41412], Loss: 2.9218, Perplexity: 18.57541\n",
      "Epoch [1/3], Step [8100/41412], Loss: 3.2556, Perplexity: 25.9364\n",
      "Epoch [1/3], Step [8200/41412], Loss: 2.6375, Perplexity: 13.9784\n",
      "Epoch [1/3], Step [8300/41412], Loss: 2.7859, Perplexity: 16.2148\n",
      "Epoch [1/3], Step [11600/41412], Loss: 3.0309, Perplexity: 20.7153\n",
      "Epoch [1/3], Step [11700/41412], Loss: 2.9885, Perplexity: 19.8564\n",
      "Epoch [1/3], Step [11800/41412], Loss: 2.4260, Perplexity: 11.3139\n",
      "Epoch [1/3], Step [11900/41412], Loss: 2.8139, Perplexity: 16.6751\n",
      "Epoch [1/3], Step [12000/41412], Loss: 2.7204, Perplexity: 15.1863\n",
      "Epoch [1/3], Step [12100/41412], Loss: 2.7877, Perplexity: 16.2444\n",
      "Epoch [1/3], Step [12200/41412], Loss: 2.9644, Perplexity: 19.3830\n",
      "Epoch [1/3], Step [12300/41412], Loss: 2.4118, Perplexity: 11.1545\n",
      "Epoch [1/3], Step [12400/41412], Loss: 2.7478, Perplexity: 15.6076\n",
      "Epoch [1/3], Step [12500/41412], Loss: 2.3580, Perplexity: 10.5699\n",
      "Epoch [1/3], Step [12600/41412], Loss: 2.8196, Perplexity: 16.7693\n",
      "Epoch [1/3], Step [12700/41412], Loss: 2.1195, Perplexity: 8.32676\n",
      "Epoch [1/3], Step [12800/41412], Loss: 2.6014, Perplexity: 13.4833\n",
      "Epoch [1/3], Step [12900/41412], Loss: 3.0799, Perplexity: 21.7555\n",
      "Epoch [1/3], Step [13000/41412], Loss: 2.6749, Perplexity: 14.5107\n",
      "Epoch [1/3], Step [13100/41412], Loss: 3.3750, Perplexity: 29.2236\n",
      "Epoch [1/3], Step [13200/41412], Loss: 2.3442, Perplexity: 10.4246\n",
      "Epoch [1/3], Step [13300/41412], Loss: 2.9190, Perplexity: 18.5220\n",
      "Epoch [1/3], Step [13400/41412], Loss: 2.6089, Perplexity: 13.5844\n",
      "Epoch [1/3], Step [13500/41412], Loss: 2.9799, Perplexity: 19.6852\n",
      "Epoch [1/3], Step [13600/41412], Loss: 3.2922, Perplexity: 26.9027\n",
      "Epoch [1/3], Step [13700/41412], Loss: 3.0638, Perplexity: 21.4085\n",
      "Epoch [1/3], Step [13800/41412], Loss: 3.0443, Perplexity: 20.9963\n",
      "Epoch [1/3], Step [13900/41412], Loss: 3.0592, Perplexity: 21.3102\n",
      "Epoch [1/3], Step [14000/41412], Loss: 3.0913, Perplexity: 22.0059\n",
      "Epoch [1/3], Step [14100/41412], Loss: 3.3954, Perplexity: 29.8264\n",
      "Epoch [1/3], Step [14200/41412], Loss: 2.6679, Perplexity: 14.4091\n",
      "Epoch [1/3], Step [14300/41412], Loss: 2.7041, Perplexity: 14.9402\n",
      "Epoch [1/3], Step [14400/41412], Loss: 2.6288, Perplexity: 13.8577\n",
      "Epoch [1/3], Step [14500/41412], Loss: 2.7458, Perplexity: 15.5773\n",
      "Epoch [1/3], Step [14600/41412], Loss: 2.1778, Perplexity: 8.827130\n",
      "Epoch [1/3], Step [14700/41412], Loss: 3.3583, Perplexity: 28.7409\n",
      "Epoch [1/3], Step [14800/41412], Loss: 3.1345, Perplexity: 22.9777\n",
      "Epoch [1/3], Step [14900/41412], Loss: 2.5058, Perplexity: 12.2532\n",
      "Epoch [1/3], Step [15000/41412], Loss: 3.2088, Perplexity: 24.7490\n",
      "Epoch [1/3], Step [15100/41412], Loss: 2.7675, Perplexity: 15.9185\n",
      "Epoch [1/3], Step [15200/41412], Loss: 2.7912, Perplexity: 16.3004\n",
      "Epoch [1/3], Step [15300/41412], Loss: 3.3548, Perplexity: 28.6404\n",
      "Epoch [1/3], Step [15400/41412], Loss: 2.8280, Perplexity: 16.9118\n",
      "Epoch [1/3], Step [15500/41412], Loss: 2.4069, Perplexity: 11.0992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [15600/41412], Loss: 2.8493, Perplexity: 17.2751\n",
      "Epoch [1/3], Step [15700/41412], Loss: 2.0323, Perplexity: 7.63170\n",
      "Epoch [1/3], Step [15800/41412], Loss: 2.5894, Perplexity: 13.3212\n",
      "Epoch [1/3], Step [15900/41412], Loss: 2.6790, Perplexity: 14.5703\n",
      "Epoch [1/3], Step [16000/41412], Loss: 2.5273, Perplexity: 12.5195\n",
      "Epoch [1/3], Step [16100/41412], Loss: 2.5746, Perplexity: 13.1265\n",
      "Epoch [1/3], Step [16200/41412], Loss: 2.5078, Perplexity: 12.2782\n",
      "Epoch [1/3], Step [16300/41412], Loss: 2.2623, Perplexity: 9.60507\n",
      "Epoch [1/3], Step [16400/41412], Loss: 2.8572, Perplexity: 17.4131\n",
      "Epoch [1/3], Step [16500/41412], Loss: 2.8820, Perplexity: 17.8501\n",
      "Epoch [1/3], Step [16600/41412], Loss: 2.3583, Perplexity: 10.5725\n",
      "Epoch [1/3], Step [16700/41412], Loss: 2.7888, Perplexity: 16.2614\n",
      "Epoch [1/3], Step [16800/41412], Loss: 3.3817, Perplexity: 29.4198\n",
      "Epoch [1/3], Step [16900/41412], Loss: 2.8491, Perplexity: 17.2714\n",
      "Epoch [1/3], Step [17000/41412], Loss: 2.4153, Perplexity: 11.1927\n",
      "Epoch [1/3], Step [17100/41412], Loss: 2.9764, Perplexity: 19.6171\n",
      "Epoch [1/3], Step [17200/41412], Loss: 2.2636, Perplexity: 9.61724\n",
      "Epoch [1/3], Step [17300/41412], Loss: 2.8754, Perplexity: 17.73294\n",
      "Epoch [1/3], Step [17400/41412], Loss: 2.0141, Perplexity: 7.49405\n",
      "Epoch [1/3], Step [17500/41412], Loss: 2.5711, Perplexity: 13.0799\n",
      "Epoch [1/3], Step [17600/41412], Loss: 3.2894, Perplexity: 26.8259\n",
      "Epoch [1/3], Step [17700/41412], Loss: 2.3574, Perplexity: 10.5635\n",
      "Epoch [1/3], Step [17800/41412], Loss: 2.5807, Perplexity: 13.2059\n",
      "Epoch [1/3], Step [17900/41412], Loss: 2.7617, Perplexity: 15.8274\n",
      "Epoch [1/3], Step [18000/41412], Loss: 2.8600, Perplexity: 17.4617\n",
      "Epoch [1/3], Step [18100/41412], Loss: 2.4248, Perplexity: 11.3002\n",
      "Epoch [1/3], Step [18200/41412], Loss: 2.8257, Perplexity: 16.8728\n",
      "Epoch [1/3], Step [18300/41412], Loss: 3.3825, Perplexity: 29.4434\n",
      "Epoch [1/3], Step [18400/41412], Loss: 2.3712, Perplexity: 10.7102\n",
      "Epoch [1/3], Step [18500/41412], Loss: 2.9490, Perplexity: 19.0861\n",
      "Epoch [1/3], Step [18600/41412], Loss: 2.7430, Perplexity: 15.5333\n",
      "Epoch [1/3], Step [18700/41412], Loss: 3.4514, Perplexity: 31.5440\n",
      "Epoch [1/3], Step [18800/41412], Loss: 2.2111, Perplexity: 9.12537\n",
      "Epoch [1/3], Step [18900/41412], Loss: 2.3320, Perplexity: 10.2983\n",
      "Epoch [1/3], Step [19000/41412], Loss: 2.8158, Perplexity: 16.7072\n",
      "Epoch [1/3], Step [19100/41412], Loss: 2.7237, Perplexity: 15.2363\n",
      "Epoch [1/3], Step [19200/41412], Loss: 2.6889, Perplexity: 14.7162\n",
      "Epoch [1/3], Step [19300/41412], Loss: 2.9083, Perplexity: 18.3254\n",
      "Epoch [1/3], Step [19400/41412], Loss: 3.0116, Perplexity: 20.3205\n",
      "Epoch [1/3], Step [19500/41412], Loss: 2.7365, Perplexity: 15.4321\n",
      "Epoch [1/3], Step [19600/41412], Loss: 2.5791, Perplexity: 13.1850\n",
      "Epoch [1/3], Step [19700/41412], Loss: 2.6759, Perplexity: 14.5253\n",
      "Epoch [1/3], Step [19800/41412], Loss: 2.1842, Perplexity: 8.884012\n",
      "Epoch [1/3], Step [19900/41412], Loss: 2.8129, Perplexity: 16.6575\n",
      "Epoch [1/3], Step [20000/41412], Loss: 2.7337, Perplexity: 15.3894\n",
      "Epoch [1/3], Step [20100/41412], Loss: 2.2584, Perplexity: 9.567920\n",
      "Epoch [1/3], Step [20200/41412], Loss: 2.5432, Perplexity: 12.7206\n",
      "Epoch [1/3], Step [20300/41412], Loss: 2.3626, Perplexity: 10.6184\n",
      "Epoch [1/3], Step [20400/41412], Loss: 2.4171, Perplexity: 11.2131\n",
      "Epoch [1/3], Step [20500/41412], Loss: 2.9628, Perplexity: 19.35263\n",
      "Epoch [1/3], Step [20600/41412], Loss: 3.5164, Perplexity: 33.66279\n",
      "Epoch [1/3], Step [20700/41412], Loss: 2.9388, Perplexity: 18.8929\n",
      "Epoch [1/3], Step [20800/41412], Loss: 2.6903, Perplexity: 14.7355\n",
      "Epoch [1/3], Step [20900/41412], Loss: 2.8068, Perplexity: 16.55732\n",
      "Epoch [1/3], Step [21000/41412], Loss: 2.9782, Perplexity: 19.6534\n",
      "Epoch [1/3], Step [21100/41412], Loss: 2.8562, Perplexity: 17.3945\n",
      "Epoch [1/3], Step [21200/41412], Loss: 2.3511, Perplexity: 10.4972\n",
      "Epoch [1/3], Step [21300/41412], Loss: 2.0840, Perplexity: 8.03628\n",
      "Epoch [1/3], Step [21400/41412], Loss: 2.7855, Perplexity: 16.2087\n",
      "Epoch [1/3], Step [21500/41412], Loss: 2.1903, Perplexity: 8.93777\n",
      "Epoch [1/3], Step [21600/41412], Loss: 2.6608, Perplexity: 14.3081\n",
      "Epoch [1/3], Step [21700/41412], Loss: 2.5637, Perplexity: 12.9838\n",
      "Epoch [1/3], Step [21800/41412], Loss: 2.5994, Perplexity: 13.4553\n",
      "Epoch [1/3], Step [21900/41412], Loss: 2.9130, Perplexity: 18.4113\n",
      "Epoch [1/3], Step [22000/41412], Loss: 2.3049, Perplexity: 10.0236\n",
      "Epoch [1/3], Step [22100/41412], Loss: 2.5707, Perplexity: 13.0748\n",
      "Epoch [1/3], Step [22200/41412], Loss: 2.5378, Perplexity: 12.6515\n",
      "Epoch [1/3], Step [22300/41412], Loss: 2.2885, Perplexity: 9.86038\n",
      "Epoch [1/3], Step [22400/41412], Loss: 2.2019, Perplexity: 9.04247\n",
      "Epoch [1/3], Step [22500/41412], Loss: 2.2966, Perplexity: 9.94000\n",
      "Epoch [1/3], Step [22600/41412], Loss: 2.6214, Perplexity: 13.7553\n",
      "Epoch [1/3], Step [22700/41412], Loss: 2.4564, Perplexity: 11.6627\n",
      "Epoch [1/3], Step [22800/41412], Loss: 2.3563, Perplexity: 10.5517\n",
      "Epoch [1/3], Step [22900/41412], Loss: 3.0450, Perplexity: 21.0099\n",
      "Epoch [1/3], Step [23000/41412], Loss: 2.9649, Perplexity: 19.3929\n",
      "Epoch [1/3], Step [23100/41412], Loss: 2.5389, Perplexity: 12.6655\n",
      "Epoch [1/3], Step [23200/41412], Loss: 2.5004, Perplexity: 12.1868\n",
      "Epoch [1/3], Step [23300/41412], Loss: 3.0357, Perplexity: 20.81496\n",
      "Epoch [1/3], Step [23400/41412], Loss: 3.2540, Perplexity: 25.8934\n",
      "Epoch [1/3], Step [23500/41412], Loss: 2.3683, Perplexity: 10.6788\n",
      "Epoch [1/3], Step [23600/41412], Loss: 2.8051, Perplexity: 16.5285\n",
      "Epoch [1/3], Step [23700/41412], Loss: 2.5191, Perplexity: 12.4175\n",
      "Epoch [1/3], Step [23800/41412], Loss: 3.2089, Perplexity: 24.7507\n",
      "Epoch [1/3], Step [23900/41412], Loss: 3.1830, Perplexity: 24.1196\n",
      "Epoch [1/3], Step [24000/41412], Loss: 2.5901, Perplexity: 13.3312\n",
      "Epoch [1/3], Step [24100/41412], Loss: 2.4942, Perplexity: 12.1117\n",
      "Epoch [1/3], Step [24200/41412], Loss: 2.9892, Perplexity: 19.8704\n",
      "Epoch [1/3], Step [24300/41412], Loss: 3.2154, Perplexity: 24.9141\n",
      "Epoch [1/3], Step [24400/41412], Loss: 2.8297, Perplexity: 16.9411\n",
      "Epoch [1/3], Step [24500/41412], Loss: 3.0616, Perplexity: 21.3625\n",
      "Epoch [1/3], Step [24600/41412], Loss: 2.3914, Perplexity: 10.9287\n",
      "Epoch [1/3], Step [24700/41412], Loss: 2.2006, Perplexity: 9.03076\n",
      "Epoch [1/3], Step [24800/41412], Loss: 2.9122, Perplexity: 18.39811\n",
      "Epoch [1/3], Step [24900/41412], Loss: 2.3551, Perplexity: 10.5396\n",
      "Epoch [1/3], Step [25000/41412], Loss: 2.4428, Perplexity: 11.5054\n",
      "Epoch [1/3], Step [25100/41412], Loss: 2.6846, Perplexity: 14.6529\n",
      "Epoch [1/3], Step [25200/41412], Loss: 1.9875, Perplexity: 7.29731\n",
      "Epoch [1/3], Step [25300/41412], Loss: 2.7249, Perplexity: 15.2555\n",
      "Epoch [1/3], Step [25400/41412], Loss: 2.4625, Perplexity: 11.7339\n",
      "Epoch [1/3], Step [25500/41412], Loss: 2.6041, Perplexity: 13.5184\n",
      "Epoch [1/3], Step [25600/41412], Loss: 3.4170, Perplexity: 30.4773\n",
      "Epoch [1/3], Step [25700/41412], Loss: 2.3883, Perplexity: 10.8945\n",
      "Epoch [1/3], Step [25800/41412], Loss: 2.3870, Perplexity: 10.8809\n",
      "Epoch [1/3], Step [25900/41412], Loss: 2.7874, Perplexity: 16.2391\n",
      "Epoch [1/3], Step [26000/41412], Loss: 2.9249, Perplexity: 18.6327\n",
      "Epoch [1/3], Step [26100/41412], Loss: 1.9610, Perplexity: 7.10677\n",
      "Epoch [1/3], Step [26200/41412], Loss: 2.8194, Perplexity: 16.7665\n",
      "Epoch [1/3], Step [26300/41412], Loss: 2.4002, Perplexity: 11.0249\n",
      "Epoch [1/3], Step [26400/41412], Loss: 2.2314, Perplexity: 9.31319\n",
      "Epoch [1/3], Step [26500/41412], Loss: 2.8260, Perplexity: 16.8785\n",
      "Epoch [1/3], Step [26600/41412], Loss: 2.8247, Perplexity: 16.8556\n",
      "Epoch [1/3], Step [26700/41412], Loss: 2.8938, Perplexity: 18.06121\n",
      "Epoch [1/3], Step [26800/41412], Loss: 2.7804, Perplexity: 16.1247\n",
      "Epoch [1/3], Step [26900/41412], Loss: 2.8539, Perplexity: 17.3558\n",
      "Epoch [1/3], Step [27000/41412], Loss: 2.7437, Perplexity: 15.5439\n",
      "Epoch [1/3], Step [27100/41412], Loss: 2.7029, Perplexity: 14.9226\n",
      "Epoch [1/3], Step [27200/41412], Loss: 2.5981, Perplexity: 13.4385\n",
      "Epoch [1/3], Step [27300/41412], Loss: 2.1869, Perplexity: 8.90767\n",
      "Epoch [1/3], Step [27400/41412], Loss: 3.2790, Perplexity: 26.5482\n",
      "Epoch [1/3], Step [27500/41412], Loss: 2.8276, Perplexity: 16.9050\n",
      "Epoch [1/3], Step [27600/41412], Loss: 2.1330, Perplexity: 8.43997\n",
      "Epoch [1/3], Step [27700/41412], Loss: 2.8955, Perplexity: 18.0928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [27800/41412], Loss: 2.7159, Perplexity: 15.1177\n",
      "Epoch [1/3], Step [27900/41412], Loss: 2.9484, Perplexity: 19.0757\n",
      "Epoch [1/3], Step [28000/41412], Loss: 2.3872, Perplexity: 10.8833\n",
      "Epoch [1/3], Step [28100/41412], Loss: 2.8128, Perplexity: 16.6569\n",
      "Epoch [1/3], Step [28200/41412], Loss: 2.1302, Perplexity: 8.41670\n",
      "Epoch [1/3], Step [28300/41412], Loss: 2.0719, Perplexity: 7.93991\n",
      "Epoch [1/3], Step [28400/41412], Loss: 2.5823, Perplexity: 13.2272\n",
      "Epoch [1/3], Step [28500/41412], Loss: 2.7802, Perplexity: 16.1219\n",
      "Epoch [1/3], Step [28600/41412], Loss: 3.1719, Perplexity: 23.8526\n",
      "Epoch [1/3], Step [28700/41412], Loss: 2.5481, Perplexity: 12.7830\n",
      "Epoch [1/3], Step [28800/41412], Loss: 2.4110, Perplexity: 11.1447\n",
      "Epoch [1/3], Step [28900/41412], Loss: 1.8679, Perplexity: 6.47449\n",
      "Epoch [1/3], Step [29000/41412], Loss: 3.1380, Perplexity: 23.0583\n",
      "Epoch [1/3], Step [29100/41412], Loss: 2.4752, Perplexity: 11.8836\n",
      "Epoch [1/3], Step [29200/41412], Loss: 2.9181, Perplexity: 18.5062\n",
      "Epoch [1/3], Step [29300/41412], Loss: 2.3245, Perplexity: 10.2212\n",
      "Epoch [1/3], Step [29400/41412], Loss: 2.4111, Perplexity: 11.1458\n",
      "Epoch [1/3], Step [29500/41412], Loss: 1.9398, Perplexity: 6.95712\n",
      "Epoch [1/3], Step [29600/41412], Loss: 2.4420, Perplexity: 11.4963\n",
      "Epoch [1/3], Step [29700/41412], Loss: 2.7972, Perplexity: 16.3981\n",
      "Epoch [1/3], Step [29800/41412], Loss: 2.4395, Perplexity: 11.4669\n",
      "Epoch [1/3], Step [29900/41412], Loss: 2.2261, Perplexity: 9.26338\n",
      "Epoch [1/3], Step [30000/41412], Loss: 2.3247, Perplexity: 10.2238\n",
      "Epoch [1/3], Step [30100/41412], Loss: 2.2484, Perplexity: 9.47309\n",
      "Epoch [1/3], Step [30200/41412], Loss: 3.2346, Perplexity: 25.3965\n",
      "Epoch [1/3], Step [30300/41412], Loss: 2.6883, Perplexity: 14.7059\n",
      "Epoch [1/3], Step [30400/41412], Loss: 2.6618, Perplexity: 14.3225\n",
      "Epoch [1/3], Step [30500/41412], Loss: 2.5525, Perplexity: 12.8394\n",
      "Epoch [1/3], Step [30600/41412], Loss: 2.3194, Perplexity: 10.1692\n",
      "Epoch [1/3], Step [30700/41412], Loss: 2.3884, Perplexity: 10.8963\n",
      "Epoch [1/3], Step [30800/41412], Loss: 2.3528, Perplexity: 10.5145\n",
      "Epoch [1/3], Step [30900/41412], Loss: 3.3546, Perplexity: 28.6343\n",
      "Epoch [1/3], Step [31000/41412], Loss: 3.2473, Perplexity: 25.7213\n",
      "Epoch [1/3], Step [31100/41412], Loss: 2.3332, Perplexity: 10.3107\n",
      "Epoch [1/3], Step [31200/41412], Loss: 2.6744, Perplexity: 14.5030\n",
      "Epoch [1/3], Step [31300/41412], Loss: 2.3176, Perplexity: 10.1510\n",
      "Epoch [1/3], Step [31400/41412], Loss: 2.6332, Perplexity: 13.91840\n",
      "Epoch [1/3], Step [31500/41412], Loss: 2.3424, Perplexity: 10.4061\n",
      "Epoch [1/3], Step [31600/41412], Loss: 1.9335, Perplexity: 6.91391\n",
      "Epoch [1/3], Step [31700/41412], Loss: 2.2771, Perplexity: 9.74889\n",
      "Epoch [1/3], Step [31800/41412], Loss: 2.5723, Perplexity: 13.0964\n",
      "Epoch [1/3], Step [31900/41412], Loss: 2.2679, Perplexity: 9.65915\n",
      "Epoch [1/3], Step [32000/41412], Loss: 2.3681, Perplexity: 10.6769\n",
      "Epoch [1/3], Step [32100/41412], Loss: 2.4617, Perplexity: 11.7251\n",
      "Epoch [1/3], Step [32200/41412], Loss: 2.8942, Perplexity: 18.0693\n",
      "Epoch [1/3], Step [32300/41412], Loss: 2.2990, Perplexity: 9.964476\n",
      "Epoch [1/3], Step [32400/41412], Loss: 2.0372, Perplexity: 7.66889\n",
      "Epoch [1/3], Step [32500/41412], Loss: 2.1075, Perplexity: 8.22743\n",
      "Epoch [1/3], Step [32600/41412], Loss: 2.2383, Perplexity: 9.37771\n",
      "Epoch [1/3], Step [32700/41412], Loss: 2.4689, Perplexity: 11.8097\n",
      "Epoch [1/3], Step [32800/41412], Loss: 2.3684, Perplexity: 10.67987\n",
      "Epoch [1/3], Step [32900/41412], Loss: 2.0759, Perplexity: 7.97178\n",
      "Epoch [1/3], Step [33000/41412], Loss: 3.1094, Perplexity: 22.4065\n",
      "Epoch [1/3], Step [33100/41412], Loss: 2.7419, Perplexity: 15.5161\n",
      "Epoch [1/3], Step [33200/41412], Loss: 2.7543, Perplexity: 15.7097\n",
      "Epoch [1/3], Step [33300/41412], Loss: 2.9941, Perplexity: 19.9669\n",
      "Epoch [1/3], Step [33400/41412], Loss: 3.2862, Perplexity: 26.7407\n",
      "Epoch [1/3], Step [33500/41412], Loss: 2.6034, Perplexity: 13.5096\n",
      "Epoch [1/3], Step [33600/41412], Loss: 3.0762, Perplexity: 21.6756\n",
      "Epoch [1/3], Step [33700/41412], Loss: 2.0613, Perplexity: 7.85646\n",
      "Epoch [1/3], Step [33800/41412], Loss: 2.0664, Perplexity: 7.89666\n",
      "Epoch [1/3], Step [33900/41412], Loss: 2.0540, Perplexity: 7.79906\n",
      "Epoch [1/3], Step [34000/41412], Loss: 2.7449, Perplexity: 15.5636\n",
      "Epoch [1/3], Step [34100/41412], Loss: 2.0111, Perplexity: 7.47143\n",
      "Epoch [1/3], Step [34200/41412], Loss: 2.2037, Perplexity: 9.05819\n",
      "Epoch [1/3], Step [34300/41412], Loss: 2.3912, Perplexity: 10.9261\n",
      "Epoch [1/3], Step [34400/41412], Loss: 2.3529, Perplexity: 10.5165\n",
      "Epoch [1/3], Step [34500/41412], Loss: 2.5532, Perplexity: 12.8479\n",
      "Epoch [1/3], Step [34600/41412], Loss: 1.8824, Perplexity: 6.56948\n",
      "Epoch [1/3], Step [34700/41412], Loss: 2.0964, Perplexity: 8.13703\n",
      "Epoch [1/3], Step [34800/41412], Loss: 2.2559, Perplexity: 9.54344\n",
      "Epoch [1/3], Step [34900/41412], Loss: 1.9938, Perplexity: 7.343568\n",
      "Epoch [1/3], Step [35000/41412], Loss: 2.4870, Perplexity: 12.0251\n",
      "Epoch [1/3], Step [35100/41412], Loss: 2.3038, Perplexity: 10.0123\n",
      "Epoch [1/3], Step [35200/41412], Loss: 2.4365, Perplexity: 11.4330\n",
      "Epoch [1/3], Step [35300/41412], Loss: 3.1201, Perplexity: 22.6484\n",
      "Epoch [1/3], Step [35400/41412], Loss: 2.3572, Perplexity: 10.5612\n",
      "Epoch [1/3], Step [35500/41412], Loss: 2.1040, Perplexity: 8.19874\n",
      "Epoch [1/3], Step [35600/41412], Loss: 1.9878, Perplexity: 7.29935\n",
      "Epoch [1/3], Step [35700/41412], Loss: 3.1103, Perplexity: 22.4286\n",
      "Epoch [1/3], Step [35800/41412], Loss: 2.4942, Perplexity: 12.1125\n",
      "Epoch [1/3], Step [35900/41412], Loss: 2.3777, Perplexity: 10.7796\n",
      "Epoch [1/3], Step [36000/41412], Loss: 2.3269, Perplexity: 10.2458\n",
      "Epoch [1/3], Step [36100/41412], Loss: 2.2639, Perplexity: 9.62053\n",
      "Epoch [1/3], Step [36200/41412], Loss: 2.8925, Perplexity: 18.0384\n",
      "Epoch [1/3], Step [36300/41412], Loss: 2.2529, Perplexity: 9.515367\n",
      "Epoch [1/3], Step [36400/41412], Loss: 2.1011, Perplexity: 8.17567\n",
      "Epoch [1/3], Step [36500/41412], Loss: 2.5066, Perplexity: 12.2637\n",
      "Epoch [1/3], Step [36600/41412], Loss: 2.2767, Perplexity: 9.74459\n",
      "Epoch [1/3], Step [36700/41412], Loss: 3.1239, Perplexity: 22.7351\n",
      "Epoch [1/3], Step [36800/41412], Loss: 2.9439, Perplexity: 18.9903\n",
      "Epoch [1/3], Step [36900/41412], Loss: 2.8212, Perplexity: 16.7975\n",
      "Epoch [1/3], Step [37000/41412], Loss: 2.7395, Perplexity: 15.4791\n",
      "Epoch [1/3], Step [37100/41412], Loss: 2.8852, Perplexity: 17.9071\n",
      "Epoch [1/3], Step [37200/41412], Loss: 3.0164, Perplexity: 20.4185\n",
      "Epoch [1/3], Step [37300/41412], Loss: 2.3321, Perplexity: 10.2996\n",
      "Epoch [1/3], Step [37400/41412], Loss: 2.0478, Perplexity: 7.75067\n",
      "Epoch [1/3], Step [37500/41412], Loss: 2.2028, Perplexity: 9.05079\n",
      "Epoch [1/3], Step [37600/41412], Loss: 2.3212, Perplexity: 10.1879\n",
      "Epoch [1/3], Step [37700/41412], Loss: 2.2202, Perplexity: 9.20907\n",
      "Epoch [1/3], Step [37800/41412], Loss: 1.9226, Perplexity: 6.83892\n",
      "Epoch [1/3], Step [37900/41412], Loss: 2.7217, Perplexity: 15.2056\n",
      "Epoch [1/3], Step [38000/41412], Loss: 2.2499, Perplexity: 9.48713\n",
      "Epoch [1/3], Step [38100/41412], Loss: 2.4886, Perplexity: 12.0446\n",
      "Epoch [1/3], Step [38200/41412], Loss: 2.8465, Perplexity: 17.2281\n",
      "Epoch [1/3], Step [38300/41412], Loss: 2.4971, Perplexity: 12.1474\n",
      "Epoch [1/3], Step [38400/41412], Loss: 3.0368, Perplexity: 20.8394\n",
      "Epoch [1/3], Step [38500/41412], Loss: 2.2870, Perplexity: 9.84559\n",
      "Epoch [1/3], Step [38600/41412], Loss: 1.8711, Perplexity: 6.495286\n",
      "Epoch [1/3], Step [38700/41412], Loss: 2.1645, Perplexity: 8.70982\n",
      "Epoch [1/3], Step [38800/41412], Loss: 2.1761, Perplexity: 8.81237\n",
      "Epoch [1/3], Step [38900/41412], Loss: 2.5302, Perplexity: 12.5561\n",
      "Epoch [1/3], Step [39000/41412], Loss: 2.0426, Perplexity: 7.71099\n",
      "Epoch [1/3], Step [39100/41412], Loss: 3.3518, Perplexity: 28.5545\n",
      "Epoch [1/3], Step [39200/41412], Loss: 2.3487, Perplexity: 10.4714\n",
      "Epoch [1/3], Step [39300/41412], Loss: 3.0866, Perplexity: 21.9019\n",
      "Epoch [1/3], Step [39400/41412], Loss: 2.4089, Perplexity: 11.1216\n",
      "Epoch [1/3], Step [39500/41412], Loss: 2.6263, Perplexity: 13.8220\n",
      "Epoch [1/3], Step [39600/41412], Loss: 2.1639, Perplexity: 8.70530\n",
      "Epoch [1/3], Step [39700/41412], Loss: 1.8805, Perplexity: 6.55685\n",
      "Epoch [1/3], Step [39800/41412], Loss: 2.0792, Perplexity: 7.99826\n",
      "Epoch [1/3], Step [39900/41412], Loss: 2.6429, Perplexity: 14.0536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [40000/41412], Loss: 1.9543, Perplexity: 7.05887\n",
      "Epoch [1/3], Step [40100/41412], Loss: 2.2449, Perplexity: 9.43921\n",
      "Epoch [1/3], Step [40200/41412], Loss: 1.9445, Perplexity: 6.99027\n",
      "Epoch [1/3], Step [40300/41412], Loss: 2.6486, Perplexity: 14.1349\n",
      "Epoch [1/3], Step [40400/41412], Loss: 1.9522, Perplexity: 7.04432\n",
      "Epoch [1/3], Step [40500/41412], Loss: 2.0712, Perplexity: 7.93460\n",
      "Epoch [1/3], Step [40600/41412], Loss: 2.5121, Perplexity: 12.3307\n",
      "Epoch [1/3], Step [40700/41412], Loss: 2.8482, Perplexity: 17.2561\n",
      "Epoch [1/3], Step [40800/41412], Loss: 2.9785, Perplexity: 19.6575\n",
      "Epoch [1/3], Step [40900/41412], Loss: 2.9802, Perplexity: 19.6924\n",
      "Epoch [1/3], Step [41000/41412], Loss: 2.1410, Perplexity: 8.50776\n",
      "Epoch [1/3], Step [41100/41412], Loss: 2.9400, Perplexity: 18.9161\n",
      "Epoch [1/3], Step [41200/41412], Loss: 2.9493, Perplexity: 19.0919\n",
      "Epoch [1/3], Step [41300/41412], Loss: 2.1598, Perplexity: 8.66934\n",
      "Epoch [1/3], Step [41400/41412], Loss: 2.9683, Perplexity: 19.4583\n",
      "Epoch [2/3], Step [100/41412], Loss: 2.4668, Perplexity: 11.784741\n",
      "Epoch [2/3], Step [200/41412], Loss: 2.2667, Perplexity: 9.64750\n",
      "Epoch [2/3], Step [300/41412], Loss: 2.2157, Perplexity: 9.16811\n",
      "Epoch [2/3], Step [400/41412], Loss: 2.5179, Perplexity: 12.4030\n",
      "Epoch [2/3], Step [500/41412], Loss: 2.4212, Perplexity: 11.2588\n",
      "Epoch [2/3], Step [600/41412], Loss: 2.5660, Perplexity: 13.0139\n",
      "Epoch [2/3], Step [700/41412], Loss: 2.8117, Perplexity: 16.6390\n",
      "Epoch [2/3], Step [800/41412], Loss: 2.4736, Perplexity: 11.8654\n",
      "Epoch [2/3], Step [900/41412], Loss: 2.8563, Perplexity: 17.3968\n",
      "Epoch [2/3], Step [1000/41412], Loss: 2.9304, Perplexity: 18.7346\n",
      "Epoch [2/3], Step [1100/41412], Loss: 1.7442, Perplexity: 5.72159\n",
      "Epoch [2/3], Step [1200/41412], Loss: 2.4102, Perplexity: 11.1359\n",
      "Epoch [2/3], Step [1300/41412], Loss: 2.3174, Perplexity: 10.1489\n",
      "Epoch [2/3], Step [1400/41412], Loss: 2.4666, Perplexity: 11.7824\n",
      "Epoch [2/3], Step [1500/41412], Loss: 2.7058, Perplexity: 14.9667\n",
      "Epoch [2/3], Step [1600/41412], Loss: 2.1421, Perplexity: 8.51714\n",
      "Epoch [2/3], Step [1700/41412], Loss: 1.8186, Perplexity: 6.16343\n",
      "Epoch [2/3], Step [1800/41412], Loss: 2.3759, Perplexity: 10.7606\n",
      "Epoch [2/3], Step [1900/41412], Loss: 1.9340, Perplexity: 6.91682\n",
      "Epoch [2/3], Step [2000/41412], Loss: 2.4984, Perplexity: 12.1628\n",
      "Epoch [2/3], Step [2100/41412], Loss: 2.4138, Perplexity: 11.1759\n",
      "Epoch [2/3], Step [2200/41412], Loss: 3.2990, Perplexity: 27.0854\n",
      "Epoch [2/3], Step [2300/41412], Loss: 2.5436, Perplexity: 12.7260\n",
      "Epoch [2/3], Step [2400/41412], Loss: 2.4555, Perplexity: 11.6527\n",
      "Epoch [2/3], Step [2500/41412], Loss: 2.6852, Perplexity: 14.6616\n",
      "Epoch [2/3], Step [2600/41412], Loss: 3.7718, Perplexity: 43.4562\n",
      "Epoch [2/3], Step [2700/41412], Loss: 2.6552, Perplexity: 14.2277\n",
      "Epoch [2/3], Step [2800/41412], Loss: 2.1714, Perplexity: 8.77010\n",
      "Epoch [2/3], Step [2900/41412], Loss: 1.9816, Perplexity: 7.25415\n",
      "Epoch [2/3], Step [3000/41412], Loss: 2.3080, Perplexity: 10.0539\n",
      "Epoch [2/3], Step [3100/41412], Loss: 2.6842, Perplexity: 14.6472\n",
      "Epoch [2/3], Step [3200/41412], Loss: 2.0764, Perplexity: 7.97574\n",
      "Epoch [2/3], Step [3300/41412], Loss: 2.7444, Perplexity: 15.5548\n",
      "Epoch [2/3], Step [3400/41412], Loss: 2.6998, Perplexity: 14.8768\n",
      "Epoch [2/3], Step [3500/41412], Loss: 2.3530, Perplexity: 10.5175\n",
      "Epoch [2/3], Step [3600/41412], Loss: 2.7514, Perplexity: 15.6647\n",
      "Epoch [2/3], Step [3700/41412], Loss: 2.4522, Perplexity: 11.6138\n",
      "Epoch [2/3], Step [3800/41412], Loss: 2.0051, Perplexity: 7.42725\n",
      "Epoch [2/3], Step [3900/41412], Loss: 1.9755, Perplexity: 7.21022\n",
      "Epoch [2/3], Step [4000/41412], Loss: 2.2076, Perplexity: 9.09404\n",
      "Epoch [2/3], Step [4100/41412], Loss: 2.5028, Perplexity: 12.2171\n",
      "Epoch [2/3], Step [4200/41412], Loss: 2.4614, Perplexity: 11.7208\n",
      "Epoch [2/3], Step [4300/41412], Loss: 2.7414, Perplexity: 15.5084\n",
      "Epoch [2/3], Step [4400/41412], Loss: 3.3088, Perplexity: 27.3525\n",
      "Epoch [2/3], Step [4500/41412], Loss: 1.9038, Perplexity: 6.71132\n",
      "Epoch [2/3], Step [4600/41412], Loss: 2.5222, Perplexity: 12.4563\n",
      "Epoch [2/3], Step [4700/41412], Loss: 2.8242, Perplexity: 16.8468\n",
      "Epoch [2/3], Step [4800/41412], Loss: 2.7576, Perplexity: 15.7624\n",
      "Epoch [2/3], Step [4900/41412], Loss: 2.2906, Perplexity: 9.88068\n",
      "Epoch [2/3], Step [5000/41412], Loss: 1.7439, Perplexity: 5.71953\n",
      "Epoch [2/3], Step [5100/41412], Loss: 1.9509, Perplexity: 7.03496\n",
      "Epoch [2/3], Step [5200/41412], Loss: 2.5759, Perplexity: 13.1435\n",
      "Epoch [2/3], Step [5300/41412], Loss: 3.0154, Perplexity: 20.3968\n",
      "Epoch [2/3], Step [5400/41412], Loss: 2.6613, Perplexity: 14.3143\n",
      "Epoch [2/3], Step [5500/41412], Loss: 2.1909, Perplexity: 8.94352\n",
      "Epoch [2/3], Step [5600/41412], Loss: 1.9865, Perplexity: 7.29021\n",
      "Epoch [2/3], Step [5700/41412], Loss: 2.1731, Perplexity: 8.78562\n",
      "Epoch [2/3], Step [5800/41412], Loss: 2.2765, Perplexity: 9.742702\n",
      "Epoch [2/3], Step [5900/41412], Loss: 2.6946, Perplexity: 14.8000\n",
      "Epoch [2/3], Step [6000/41412], Loss: 2.0990, Perplexity: 8.15760\n",
      "Epoch [2/3], Step [6100/41412], Loss: 2.5528, Perplexity: 12.8427\n",
      "Epoch [2/3], Step [6200/41412], Loss: 2.5318, Perplexity: 12.5757\n",
      "Epoch [2/3], Step [6300/41412], Loss: 2.7625, Perplexity: 15.8387\n",
      "Epoch [2/3], Step [6400/41412], Loss: 2.4396, Perplexity: 11.4685\n",
      "Epoch [2/3], Step [6500/41412], Loss: 2.2172, Perplexity: 9.18152\n",
      "Epoch [2/3], Step [6600/41412], Loss: 2.1958, Perplexity: 8.98764\n",
      "Epoch [2/3], Step [6700/41412], Loss: 2.6970, Perplexity: 14.8355\n",
      "Epoch [2/3], Step [6800/41412], Loss: 2.5082, Perplexity: 12.28313\n",
      "Epoch [2/3], Step [6900/41412], Loss: 1.9465, Perplexity: 7.00384\n",
      "Epoch [2/3], Step [7000/41412], Loss: 2.2751, Perplexity: 9.72881\n",
      "Epoch [2/3], Step [7100/41412], Loss: 2.3298, Perplexity: 10.2756\n",
      "Epoch [2/3], Step [7200/41412], Loss: 3.0170, Perplexity: 20.4308\n",
      "Epoch [2/3], Step [7300/41412], Loss: 2.0944, Perplexity: 8.12078\n",
      "Epoch [2/3], Step [7400/41412], Loss: 2.0914, Perplexity: 8.09638\n",
      "Epoch [2/3], Step [7500/41412], Loss: 2.0030, Perplexity: 7.41162\n",
      "Epoch [2/3], Step [7600/41412], Loss: 2.0760, Perplexity: 7.97249\n",
      "Epoch [2/3], Step [7700/41412], Loss: 2.0884, Perplexity: 8.07214\n",
      "Epoch [2/3], Step [7800/41412], Loss: 2.0766, Perplexity: 7.97743\n",
      "Epoch [2/3], Step [7900/41412], Loss: 2.1802, Perplexity: 8.84782\n",
      "Epoch [2/3], Step [8000/41412], Loss: 1.8168, Perplexity: 6.15246\n",
      "Epoch [2/3], Step [8100/41412], Loss: 1.8335, Perplexity: 6.25593\n",
      "Epoch [2/3], Step [8200/41412], Loss: 2.4227, Perplexity: 11.2761\n",
      "Epoch [2/3], Step [8300/41412], Loss: 2.9820, Perplexity: 19.7278\n",
      "Epoch [2/3], Step [8400/41412], Loss: 2.2504, Perplexity: 9.49138\n",
      "Epoch [2/3], Step [8500/41412], Loss: 2.2495, Perplexity: 9.48254\n",
      "Epoch [2/3], Step [8600/41412], Loss: 2.0382, Perplexity: 7.67708\n",
      "Epoch [2/3], Step [8700/41412], Loss: 2.4163, Perplexity: 11.2045\n",
      "Epoch [2/3], Step [8800/41412], Loss: 2.1595, Perplexity: 8.66693\n",
      "Epoch [2/3], Step [8900/41412], Loss: 2.5184, Perplexity: 12.4091\n",
      "Epoch [2/3], Step [9000/41412], Loss: 2.5119, Perplexity: 12.3285\n",
      "Epoch [2/3], Step [9100/41412], Loss: 2.3337, Perplexity: 10.3156\n",
      "Epoch [2/3], Step [9200/41412], Loss: 2.2131, Perplexity: 9.14407\n",
      "Epoch [2/3], Step [9300/41412], Loss: 2.5279, Perplexity: 12.5271\n",
      "Epoch [2/3], Step [9400/41412], Loss: 2.7442, Perplexity: 15.5528\n",
      "Epoch [2/3], Step [9500/41412], Loss: 2.3134, Perplexity: 10.1083\n",
      "Epoch [2/3], Step [9600/41412], Loss: 2.2442, Perplexity: 9.43279\n",
      "Epoch [2/3], Step [9700/41412], Loss: 2.7252, Perplexity: 15.2602\n",
      "Epoch [2/3], Step [9800/41412], Loss: 2.6152, Perplexity: 13.6701\n",
      "Epoch [2/3], Step [9900/41412], Loss: 2.2138, Perplexity: 9.15060\n",
      "Epoch [2/3], Step [10000/41412], Loss: 2.2593, Perplexity: 9.5768\n",
      "Epoch [2/3], Step [10100/41412], Loss: 2.4166, Perplexity: 11.2079\n",
      "Epoch [2/3], Step [10200/41412], Loss: 2.1543, Perplexity: 8.62157\n",
      "Epoch [2/3], Step [10300/41412], Loss: 2.8558, Perplexity: 17.3877\n",
      "Epoch [2/3], Step [10400/41412], Loss: 1.8660, Perplexity: 6.46219\n",
      "Epoch [2/3], Step [10500/41412], Loss: 2.5909, Perplexity: 13.3414\n",
      "Epoch [2/3], Step [10600/41412], Loss: 2.4249, Perplexity: 11.3017\n",
      "Epoch [2/3], Step [10700/41412], Loss: 1.9868, Perplexity: 7.29208\n",
      "Epoch [2/3], Step [10800/41412], Loss: 2.1383, Perplexity: 8.48524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [10900/41412], Loss: 2.8104, Perplexity: 16.6169\n",
      "Epoch [2/3], Step [11000/41412], Loss: 2.1137, Perplexity: 8.27858\n",
      "Epoch [2/3], Step [11100/41412], Loss: 2.2473, Perplexity: 9.46266\n",
      "Epoch [2/3], Step [11200/41412], Loss: 2.5942, Perplexity: 13.3860\n",
      "Epoch [2/3], Step [11300/41412], Loss: 3.5333, Perplexity: 34.2381\n",
      "Epoch [2/3], Step [11400/41412], Loss: 2.4702, Perplexity: 11.8253\n",
      "Epoch [2/3], Step [11500/41412], Loss: 2.2627, Perplexity: 9.60879\n",
      "Epoch [2/3], Step [11600/41412], Loss: 2.6534, Perplexity: 14.20219\n",
      "Epoch [2/3], Step [11700/41412], Loss: 1.7662, Perplexity: 5.84876\n",
      "Epoch [2/3], Step [11800/41412], Loss: 2.7583, Perplexity: 15.7734\n",
      "Epoch [2/3], Step [11900/41412], Loss: 2.3129, Perplexity: 10.1036\n",
      "Epoch [2/3], Step [12000/41412], Loss: 2.0929, Perplexity: 8.10864\n",
      "Epoch [2/3], Step [12100/41412], Loss: 2.5025, Perplexity: 12.2125\n",
      "Epoch [2/3], Step [12200/41412], Loss: 2.9100, Perplexity: 18.3574\n",
      "Epoch [2/3], Step [12300/41412], Loss: 2.1937, Perplexity: 8.96790\n",
      "Epoch [2/3], Step [12400/41412], Loss: 1.8128, Perplexity: 6.12782\n",
      "Epoch [2/3], Step [12500/41412], Loss: 3.2125, Perplexity: 24.8410\n",
      "Epoch [2/3], Step [12600/41412], Loss: 1.9988, Perplexity: 7.38048\n",
      "Epoch [2/3], Step [12700/41412], Loss: 2.2582, Perplexity: 9.56602\n",
      "Epoch [2/3], Step [12800/41412], Loss: 2.4248, Perplexity: 11.2997\n",
      "Epoch [2/3], Step [12900/41412], Loss: 2.5048, Perplexity: 12.2410\n",
      "Epoch [2/3], Step [13000/41412], Loss: 1.8144, Perplexity: 6.13742\n",
      "Epoch [2/3], Step [13100/41412], Loss: 2.3710, Perplexity: 10.7084\n",
      "Epoch [2/3], Step [13200/41412], Loss: 2.0521, Perplexity: 7.78427\n",
      "Epoch [2/3], Step [13300/41412], Loss: 1.8950, Perplexity: 6.65274\n",
      "Epoch [2/3], Step [13400/41412], Loss: 2.3873, Perplexity: 10.8842\n",
      "Epoch [2/3], Step [13500/41412], Loss: 2.4285, Perplexity: 11.3418\n",
      "Epoch [2/3], Step [13600/41412], Loss: 2.1755, Perplexity: 8.80707\n",
      "Epoch [2/3], Step [13700/41412], Loss: 2.3403, Perplexity: 10.3842\n",
      "Epoch [2/3], Step [13800/41412], Loss: 2.3102, Perplexity: 10.0760\n",
      "Epoch [2/3], Step [13900/41412], Loss: 2.3714, Perplexity: 10.7123\n",
      "Epoch [2/3], Step [14000/41412], Loss: 2.3098, Perplexity: 10.0729\n",
      "Epoch [2/3], Step [14100/41412], Loss: 2.6220, Perplexity: 13.7625\n",
      "Epoch [2/3], Step [14200/41412], Loss: 3.4786, Perplexity: 32.4145\n",
      "Epoch [2/3], Step [14300/41412], Loss: 2.5233, Perplexity: 12.4698\n",
      "Epoch [2/3], Step [14400/41412], Loss: 1.9604, Perplexity: 7.10211\n",
      "Epoch [2/3], Step [14500/41412], Loss: 2.5399, Perplexity: 12.6779\n",
      "Epoch [2/3], Step [14600/41412], Loss: 2.4672, Perplexity: 11.7899\n",
      "Epoch [2/3], Step [14700/41412], Loss: 2.2718, Perplexity: 9.69660\n",
      "Epoch [2/3], Step [14800/41412], Loss: 2.2603, Perplexity: 9.58625\n",
      "Epoch [2/3], Step [14900/41412], Loss: 2.7371, Perplexity: 15.4417\n",
      "Epoch [2/3], Step [15000/41412], Loss: 2.1265, Perplexity: 8.38525\n",
      "Epoch [2/3], Step [15100/41412], Loss: 2.3464, Perplexity: 10.4481\n",
      "Epoch [2/3], Step [15200/41412], Loss: 2.1296, Perplexity: 8.41144\n",
      "Epoch [2/3], Step [15300/41412], Loss: 2.2467, Perplexity: 9.45630\n",
      "Epoch [2/3], Step [15400/41412], Loss: 2.2170, Perplexity: 9.18017\n",
      "Epoch [2/3], Step [15500/41412], Loss: 2.4069, Perplexity: 11.0992\n",
      "Epoch [2/3], Step [15600/41412], Loss: 2.4084, Perplexity: 11.1163\n",
      "Epoch [2/3], Step [15700/41412], Loss: 2.2326, Perplexity: 9.32386\n",
      "Epoch [2/3], Step [15800/41412], Loss: 2.9801, Perplexity: 19.69070\n",
      "Epoch [2/3], Step [15900/41412], Loss: 2.1786, Perplexity: 8.83378\n",
      "Epoch [2/3], Step [16000/41412], Loss: 2.2542, Perplexity: 9.52780\n",
      "Epoch [2/3], Step [16100/41412], Loss: 2.0207, Perplexity: 7.54358\n",
      "Epoch [2/3], Step [16200/41412], Loss: 2.1419, Perplexity: 8.51586\n",
      "Epoch [2/3], Step [16300/41412], Loss: 2.4561, Perplexity: 11.6597\n",
      "Epoch [2/3], Step [16400/41412], Loss: 2.3132, Perplexity: 10.1070\n",
      "Epoch [2/3], Step [16500/41412], Loss: 2.5885, Perplexity: 13.3096\n",
      "Epoch [2/3], Step [16600/41412], Loss: 3.2554, Perplexity: 25.9309\n",
      "Epoch [2/3], Step [16700/41412], Loss: 1.9610, Perplexity: 7.10659\n",
      "Epoch [2/3], Step [16800/41412], Loss: 2.5338, Perplexity: 12.6015\n",
      "Epoch [2/3], Step [16900/41412], Loss: 2.0540, Perplexity: 7.79938\n",
      "Epoch [2/3], Step [17000/41412], Loss: 2.3281, Perplexity: 10.2584\n",
      "Epoch [2/3], Step [17100/41412], Loss: 2.0379, Perplexity: 7.67411\n",
      "Epoch [2/3], Step [17200/41412], Loss: 2.3900, Perplexity: 10.9139\n",
      "Epoch [2/3], Step [17300/41412], Loss: 2.3272, Perplexity: 10.2488\n",
      "Epoch [2/3], Step [17400/41412], Loss: 2.2592, Perplexity: 9.57540\n",
      "Epoch [2/3], Step [17500/41412], Loss: 1.9755, Perplexity: 7.21037\n",
      "Epoch [2/3], Step [17600/41412], Loss: 2.5546, Perplexity: 12.8667\n",
      "Epoch [2/3], Step [17700/41412], Loss: 1.7767, Perplexity: 5.91023\n",
      "Epoch [2/3], Step [17800/41412], Loss: 2.3197, Perplexity: 10.1726\n",
      "Epoch [2/3], Step [17900/41412], Loss: 3.0700, Perplexity: 21.5427\n",
      "Epoch [2/3], Step [18000/41412], Loss: 2.5020, Perplexity: 12.2064\n",
      "Epoch [2/3], Step [18100/41412], Loss: 2.4634, Perplexity: 11.7443\n",
      "Epoch [2/3], Step [18200/41412], Loss: 2.7790, Perplexity: 16.1033\n",
      "Epoch [2/3], Step [18300/41412], Loss: 2.2265, Perplexity: 9.26774\n",
      "Epoch [2/3], Step [18400/41412], Loss: 2.4699, Perplexity: 11.8215\n",
      "Epoch [2/3], Step [18500/41412], Loss: 2.3301, Perplexity: 10.27948\n",
      "Epoch [2/3], Step [18600/41412], Loss: 3.2883, Perplexity: 26.7986\n",
      "Epoch [2/3], Step [18700/41412], Loss: 2.3878, Perplexity: 10.8893\n",
      "Epoch [2/3], Step [18800/41412], Loss: 1.9498, Perplexity: 7.02713\n",
      "Epoch [2/3], Step [18900/41412], Loss: 3.2764, Perplexity: 26.4792\n",
      "Epoch [2/3], Step [19000/41412], Loss: 2.3133, Perplexity: 10.1081\n",
      "Epoch [2/3], Step [19100/41412], Loss: 2.8078, Perplexity: 16.5739\n",
      "Epoch [2/3], Step [19200/41412], Loss: 2.2114, Perplexity: 9.12879\n",
      "Epoch [2/3], Step [19300/41412], Loss: 2.2590, Perplexity: 9.57334\n",
      "Epoch [2/3], Step [19400/41412], Loss: 2.4967, Perplexity: 12.1422\n",
      "Epoch [2/3], Step [19500/41412], Loss: 2.6613, Perplexity: 14.3149\n",
      "Epoch [2/3], Step [19600/41412], Loss: 2.0444, Perplexity: 7.72496\n",
      "Epoch [2/3], Step [19700/41412], Loss: 2.2334, Perplexity: 9.33152\n",
      "Epoch [2/3], Step [19800/41412], Loss: 2.0117, Perplexity: 7.47584\n",
      "Epoch [2/3], Step [19900/41412], Loss: 3.2499, Perplexity: 25.7886\n",
      "Epoch [2/3], Step [20000/41412], Loss: 1.5984, Perplexity: 4.94491\n",
      "Epoch [2/3], Step [20100/41412], Loss: 1.6807, Perplexity: 5.36927\n",
      "Epoch [2/3], Step [20200/41412], Loss: 3.4436, Perplexity: 31.2983\n",
      "Epoch [2/3], Step [20300/41412], Loss: 2.4783, Perplexity: 11.9214\n",
      "Epoch [2/3], Step [20400/41412], Loss: 2.4539, Perplexity: 11.6337\n",
      "Epoch [2/3], Step [20500/41412], Loss: 1.6803, Perplexity: 5.36727\n",
      "Epoch [2/3], Step [20600/41412], Loss: 2.0437, Perplexity: 7.71876\n",
      "Epoch [2/3], Step [20700/41412], Loss: 2.4890, Perplexity: 12.0493\n",
      "Epoch [2/3], Step [20800/41412], Loss: 2.3220, Perplexity: 10.1964\n",
      "Epoch [2/3], Step [20900/41412], Loss: 2.2007, Perplexity: 9.03109\n",
      "Epoch [2/3], Step [21000/41412], Loss: 2.2738, Perplexity: 9.71597\n",
      "Epoch [2/3], Step [21100/41412], Loss: 2.5098, Perplexity: 12.3020\n",
      "Epoch [2/3], Step [21200/41412], Loss: 2.3244, Perplexity: 10.2207\n",
      "Epoch [2/3], Step [21300/41412], Loss: 2.6182, Perplexity: 13.7111\n",
      "Epoch [2/3], Step [21400/41412], Loss: 2.6761, Perplexity: 14.5284\n",
      "Epoch [2/3], Step [21500/41412], Loss: 2.6173, Perplexity: 13.6984\n",
      "Epoch [2/3], Step [21600/41412], Loss: 1.8664, Perplexity: 6.46530\n",
      "Epoch [2/3], Step [21700/41412], Loss: 2.2718, Perplexity: 9.69650\n",
      "Epoch [2/3], Step [21800/41412], Loss: 2.7668, Perplexity: 15.9082\n",
      "Epoch [2/3], Step [21900/41412], Loss: 1.8952, Perplexity: 6.65373\n",
      "Epoch [2/3], Step [22000/41412], Loss: 2.6270, Perplexity: 13.8325\n",
      "Epoch [2/3], Step [22100/41412], Loss: 3.1430, Perplexity: 23.1738\n",
      "Epoch [2/3], Step [22200/41412], Loss: 2.2833, Perplexity: 9.80919\n",
      "Epoch [2/3], Step [22300/41412], Loss: 2.8325, Perplexity: 16.9873\n",
      "Epoch [2/3], Step [22400/41412], Loss: 2.2656, Perplexity: 9.63673\n",
      "Epoch [2/3], Step [22500/41412], Loss: 2.3772, Perplexity: 10.7743\n",
      "Epoch [2/3], Step [22600/41412], Loss: 2.5323, Perplexity: 12.5828\n",
      "Epoch [2/3], Step [22700/41412], Loss: 2.6740, Perplexity: 14.4981\n",
      "Epoch [2/3], Step [22800/41412], Loss: 2.3530, Perplexity: 10.51708\n",
      "Epoch [2/3], Step [22900/41412], Loss: 2.0376, Perplexity: 7.67221\n",
      "Epoch [2/3], Step [23000/41412], Loss: 2.0804, Perplexity: 8.00790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [23100/41412], Loss: 2.4266, Perplexity: 11.3203\n",
      "Epoch [2/3], Step [23200/41412], Loss: 2.4887, Perplexity: 12.0459\n",
      "Epoch [2/3], Step [23300/41412], Loss: 2.0618, Perplexity: 7.85999\n",
      "Epoch [2/3], Step [23400/41412], Loss: 3.1612, Perplexity: 23.5979\n",
      "Epoch [2/3], Step [23500/41412], Loss: 2.2746, Perplexity: 9.72416\n",
      "Epoch [2/3], Step [23600/41412], Loss: 2.6689, Perplexity: 14.4235\n",
      "Epoch [2/3], Step [23700/41412], Loss: 2.4531, Perplexity: 11.6242\n",
      "Epoch [2/3], Step [23800/41412], Loss: 2.1933, Perplexity: 8.96481\n",
      "Epoch [2/3], Step [23900/41412], Loss: 2.4230, Perplexity: 11.2796\n",
      "Epoch [2/3], Step [24000/41412], Loss: 2.6632, Perplexity: 14.3419\n",
      "Epoch [2/3], Step [24100/41412], Loss: 2.0812, Perplexity: 8.01451\n",
      "Epoch [2/3], Step [24200/41412], Loss: 2.6499, Perplexity: 14.1528\n",
      "Epoch [2/3], Step [24300/41412], Loss: 3.0858, Perplexity: 21.8859\n",
      "Epoch [2/3], Step [24400/41412], Loss: 2.2744, Perplexity: 9.72234\n",
      "Epoch [2/3], Step [24500/41412], Loss: 2.4573, Perplexity: 11.6727\n",
      "Epoch [2/3], Step [24600/41412], Loss: 3.1704, Perplexity: 23.8165\n",
      "Epoch [2/3], Step [24700/41412], Loss: 2.3766, Perplexity: 10.7680\n",
      "Epoch [2/3], Step [24800/41412], Loss: 1.7580, Perplexity: 5.80084\n",
      "Epoch [2/3], Step [24900/41412], Loss: 1.6136, Perplexity: 5.02101\n",
      "Epoch [2/3], Step [25000/41412], Loss: 2.2050, Perplexity: 9.06996\n",
      "Epoch [2/3], Step [25100/41412], Loss: 2.5349, Perplexity: 12.6154\n",
      "Epoch [2/3], Step [25200/41412], Loss: 2.0989, Perplexity: 8.157203\n",
      "Epoch [2/3], Step [25300/41412], Loss: 2.2984, Perplexity: 9.95843\n",
      "Epoch [2/3], Step [25400/41412], Loss: 2.4682, Perplexity: 11.8015\n",
      "Epoch [2/3], Step [25500/41412], Loss: 2.0916, Perplexity: 8.09765\n",
      "Epoch [2/3], Step [25600/41412], Loss: 2.7267, Perplexity: 15.2825\n",
      "Epoch [2/3], Step [25700/41412], Loss: 2.8075, Perplexity: 16.5690\n",
      "Epoch [2/3], Step [25800/41412], Loss: 2.3168, Perplexity: 10.1428\n",
      "Epoch [2/3], Step [25900/41412], Loss: 2.4720, Perplexity: 11.8466\n",
      "Epoch [2/3], Step [26000/41412], Loss: 2.2479, Perplexity: 9.46778\n",
      "Epoch [2/3], Step [26100/41412], Loss: 2.5670, Perplexity: 13.0268\n",
      "Epoch [2/3], Step [26200/41412], Loss: 2.3611, Perplexity: 10.6023\n",
      "Epoch [2/3], Step [26300/41412], Loss: 2.3337, Perplexity: 10.3158\n",
      "Epoch [2/3], Step [26400/41412], Loss: 2.1509, Perplexity: 8.59235\n",
      "Epoch [2/3], Step [26500/41412], Loss: 2.3146, Perplexity: 10.1206\n",
      "Epoch [2/3], Step [26600/41412], Loss: 2.0808, Perplexity: 8.01088\n",
      "Epoch [2/3], Step [26700/41412], Loss: 1.6909, Perplexity: 5.42428\n",
      "Epoch [2/3], Step [26800/41412], Loss: 1.8955, Perplexity: 6.65594\n",
      "Epoch [2/3], Step [26900/41412], Loss: 2.4396, Perplexity: 11.4689\n",
      "Epoch [2/3], Step [27000/41412], Loss: 2.7759, Perplexity: 16.0539\n",
      "Epoch [2/3], Step [27100/41412], Loss: 2.6097, Perplexity: 13.5953\n",
      "Epoch [2/3], Step [27200/41412], Loss: 2.9470, Perplexity: 19.0478\n",
      "Epoch [2/3], Step [27300/41412], Loss: 2.0640, Perplexity: 7.87716\n",
      "Epoch [2/3], Step [27400/41412], Loss: 2.4097, Perplexity: 11.1307\n",
      "Epoch [2/3], Step [27500/41412], Loss: 2.7367, Perplexity: 15.4352\n",
      "Epoch [2/3], Step [27600/41412], Loss: 1.8827, Perplexity: 6.57108\n",
      "Epoch [2/3], Step [27700/41412], Loss: 2.7769, Perplexity: 16.0692\n",
      "Epoch [2/3], Step [27800/41412], Loss: 2.2215, Perplexity: 9.22153\n",
      "Epoch [2/3], Step [27900/41412], Loss: 2.1559, Perplexity: 8.63544\n",
      "Epoch [2/3], Step [28000/41412], Loss: 2.2868, Perplexity: 9.84347\n",
      "Epoch [2/3], Step [28100/41412], Loss: 2.1808, Perplexity: 8.85379\n",
      "Epoch [2/3], Step [28200/41412], Loss: 3.0267, Perplexity: 20.6292\n",
      "Epoch [2/3], Step [28300/41412], Loss: 2.3795, Perplexity: 10.7998\n",
      "Epoch [2/3], Step [28400/41412], Loss: 2.3049, Perplexity: 10.0235\n",
      "Epoch [2/3], Step [28500/41412], Loss: 2.0954, Perplexity: 8.12847\n",
      "Epoch [2/3], Step [28600/41412], Loss: 1.9619, Perplexity: 7.11280\n",
      "Epoch [2/3], Step [28700/41412], Loss: 1.9250, Perplexity: 6.85518\n",
      "Epoch [2/3], Step [28800/41412], Loss: 2.4734, Perplexity: 11.8632\n",
      "Epoch [2/3], Step [28900/41412], Loss: 2.8513, Perplexity: 17.3103\n",
      "Epoch [2/3], Step [29000/41412], Loss: 2.3725, Perplexity: 10.7245\n",
      "Epoch [2/3], Step [29100/41412], Loss: 2.6918, Perplexity: 14.7581\n",
      "Epoch [2/3], Step [29200/41412], Loss: 3.1994, Perplexity: 24.5184\n",
      "Epoch [2/3], Step [29300/41412], Loss: 3.1133, Perplexity: 22.4956\n",
      "Epoch [2/3], Step [29400/41412], Loss: 1.9266, Perplexity: 6.86630\n",
      "Epoch [2/3], Step [29500/41412], Loss: 2.8161, Perplexity: 16.7118\n",
      "Epoch [2/3], Step [29600/41412], Loss: 2.3068, Perplexity: 10.0419\n",
      "Epoch [2/3], Step [29700/41412], Loss: 2.7948, Perplexity: 16.3596\n",
      "Epoch [2/3], Step [29800/41412], Loss: 2.4306, Perplexity: 11.36595\n",
      "Epoch [2/3], Step [29900/41412], Loss: 2.6654, Perplexity: 14.3740\n",
      "Epoch [2/3], Step [30000/41412], Loss: 2.5237, Perplexity: 12.4745\n",
      "Epoch [2/3], Step [30100/41412], Loss: 2.2331, Perplexity: 9.32903\n",
      "Epoch [2/3], Step [30200/41412], Loss: 1.9025, Perplexity: 6.70246\n",
      "Epoch [2/3], Step [30300/41412], Loss: 2.1525, Perplexity: 8.60618\n",
      "Epoch [2/3], Step [30400/41412], Loss: 1.9070, Perplexity: 6.73288\n",
      "Epoch [2/3], Step [30500/41412], Loss: 3.0042, Perplexity: 20.1705\n",
      "Epoch [2/3], Step [30600/41412], Loss: 2.7038, Perplexity: 14.9364\n",
      "Epoch [2/3], Step [30700/41412], Loss: 2.6447, Perplexity: 14.0795\n",
      "Epoch [2/3], Step [30800/41412], Loss: 2.1498, Perplexity: 8.58356\n",
      "Epoch [2/3], Step [30900/41412], Loss: 2.6813, Perplexity: 14.6045\n",
      "Epoch [2/3], Step [31000/41412], Loss: 2.1699, Perplexity: 8.75705\n",
      "Epoch [2/3], Step [31100/41412], Loss: 2.3106, Perplexity: 10.0801\n",
      "Epoch [2/3], Step [31200/41412], Loss: 2.0515, Perplexity: 7.77943\n",
      "Epoch [2/3], Step [31300/41412], Loss: 2.3331, Perplexity: 10.3097\n",
      "Epoch [2/3], Step [31400/41412], Loss: 2.1706, Perplexity: 8.76381\n",
      "Epoch [2/3], Step [31500/41412], Loss: 2.6455, Perplexity: 14.0908\n",
      "Epoch [2/3], Step [31600/41412], Loss: 1.6365, Perplexity: 5.13740\n",
      "Epoch [2/3], Step [31700/41412], Loss: 2.5090, Perplexity: 12.2931\n",
      "Epoch [2/3], Step [31800/41412], Loss: 2.3735, Perplexity: 10.7347\n",
      "Epoch [2/3], Step [31900/41412], Loss: 2.1583, Perplexity: 8.65644\n",
      "Epoch [2/3], Step [32000/41412], Loss: 2.8158, Perplexity: 16.7060\n",
      "Epoch [2/3], Step [32100/41412], Loss: 2.3434, Perplexity: 10.4162\n",
      "Epoch [2/3], Step [32200/41412], Loss: 2.7605, Perplexity: 15.8082\n",
      "Epoch [2/3], Step [32300/41412], Loss: 2.0969, Perplexity: 8.14119\n",
      "Epoch [2/3], Step [32400/41412], Loss: 2.1387, Perplexity: 8.48815\n",
      "Epoch [2/3], Step [32500/41412], Loss: 2.0883, Perplexity: 8.07081\n",
      "Epoch [2/3], Step [32600/41412], Loss: 2.5813, Perplexity: 13.2138\n",
      "Epoch [2/3], Step [32700/41412], Loss: 2.1874, Perplexity: 8.91211\n",
      "Epoch [2/3], Step [32800/41412], Loss: 2.4297, Perplexity: 11.3550\n",
      "Epoch [2/3], Step [32900/41412], Loss: 1.8855, Perplexity: 6.58993\n",
      "Epoch [2/3], Step [33000/41412], Loss: 2.0822, Perplexity: 8.02196\n",
      "Epoch [2/3], Step [33100/41412], Loss: 1.7998, Perplexity: 6.04845\n",
      "Epoch [2/3], Step [33200/41412], Loss: 2.0201, Perplexity: 7.53915\n",
      "Epoch [2/3], Step [33300/41412], Loss: 2.3110, Perplexity: 10.0846\n",
      "Epoch [2/3], Step [33400/41412], Loss: 2.2342, Perplexity: 9.33893\n",
      "Epoch [2/3], Step [33500/41412], Loss: 2.4282, Perplexity: 11.3388\n",
      "Epoch [2/3], Step [33600/41412], Loss: 2.3846, Perplexity: 10.8543\n",
      "Epoch [2/3], Step [33700/41412], Loss: 3.0520, Perplexity: 21.1582\n",
      "Epoch [2/3], Step [33800/41412], Loss: 3.0094, Perplexity: 20.2754\n",
      "Epoch [2/3], Step [33900/41412], Loss: 2.2601, Perplexity: 9.58447\n",
      "Epoch [2/3], Step [34000/41412], Loss: 2.1984, Perplexity: 9.01084\n",
      "Epoch [2/3], Step [34100/41412], Loss: 1.8657, Perplexity: 6.46022\n",
      "Epoch [2/3], Step [34200/41412], Loss: 2.6935, Perplexity: 14.7835\n",
      "Epoch [2/3], Step [34300/41412], Loss: 2.2101, Perplexity: 9.11627\n",
      "Epoch [2/3], Step [34400/41412], Loss: 2.3561, Perplexity: 10.5494\n",
      "Epoch [2/3], Step [34500/41412], Loss: 2.6355, Perplexity: 13.9500\n",
      "Epoch [2/3], Step [34600/41412], Loss: 2.2472, Perplexity: 9.46083\n",
      "Epoch [2/3], Step [34700/41412], Loss: 2.3800, Perplexity: 10.8051\n",
      "Epoch [2/3], Step [34800/41412], Loss: 2.0675, Perplexity: 7.90504\n",
      "Epoch [2/3], Step [34900/41412], Loss: 3.1869, Perplexity: 24.2137\n",
      "Epoch [2/3], Step [35000/41412], Loss: 2.4466, Perplexity: 11.5493\n",
      "Epoch [2/3], Step [35100/41412], Loss: 2.0936, Perplexity: 8.11427\n",
      "Epoch [2/3], Step [35200/41412], Loss: 2.7636, Perplexity: 15.8571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [35300/41412], Loss: 2.2821, Perplexity: 9.79748\n",
      "Epoch [2/3], Step [35400/41412], Loss: 2.6984, Perplexity: 14.8554\n",
      "Epoch [2/3], Step [35500/41412], Loss: 1.9561, Perplexity: 7.07205\n",
      "Epoch [2/3], Step [35600/41412], Loss: 2.2313, Perplexity: 9.31161\n",
      "Epoch [2/3], Step [35700/41412], Loss: 2.0698, Perplexity: 7.92357\n",
      "Epoch [2/3], Step [35800/41412], Loss: 1.9471, Perplexity: 7.00841\n",
      "Epoch [2/3], Step [35900/41412], Loss: 2.1737, Perplexity: 8.79086\n",
      "Epoch [2/3], Step [36000/41412], Loss: 2.6135, Perplexity: 13.6462\n",
      "Epoch [2/3], Step [36100/41412], Loss: 2.3231, Perplexity: 10.2076\n",
      "Epoch [2/3], Step [36200/41412], Loss: 1.7739, Perplexity: 5.894105\n",
      "Epoch [2/3], Step [36300/41412], Loss: 2.2673, Perplexity: 9.65379\n",
      "Epoch [2/3], Step [36400/41412], Loss: 2.2025, Perplexity: 9.04727\n",
      "Epoch [2/3], Step [36500/41412], Loss: 2.2696, Perplexity: 9.67518\n",
      "Epoch [2/3], Step [36600/41412], Loss: 2.5252, Perplexity: 12.4931\n",
      "Epoch [2/3], Step [36700/41412], Loss: 2.5864, Perplexity: 13.2820\n",
      "Epoch [2/3], Step [36800/41412], Loss: 2.4887, Perplexity: 12.0457\n",
      "Epoch [2/3], Step [36900/41412], Loss: 2.1741, Perplexity: 8.79446\n",
      "Epoch [2/3], Step [37000/41412], Loss: 2.6103, Perplexity: 13.60267\n",
      "Epoch [2/3], Step [37100/41412], Loss: 2.0691, Perplexity: 7.91771\n",
      "Epoch [2/3], Step [37200/41412], Loss: 1.9900, Perplexity: 7.31540\n",
      "Epoch [2/3], Step [37300/41412], Loss: 2.9165, Perplexity: 18.4763\n",
      "Epoch [2/3], Step [37400/41412], Loss: 2.5909, Perplexity: 13.3414\n",
      "Epoch [2/3], Step [37500/41412], Loss: 2.7287, Perplexity: 15.3131\n",
      "Epoch [2/3], Step [37600/41412], Loss: 2.1889, Perplexity: 8.92576\n",
      "Epoch [2/3], Step [37700/41412], Loss: 1.7751, Perplexity: 5.90097\n",
      "Epoch [2/3], Step [37800/41412], Loss: 2.3454, Perplexity: 10.4375\n",
      "Epoch [2/3], Step [37900/41412], Loss: 2.2455, Perplexity: 9.44540\n",
      "Epoch [2/3], Step [38000/41412], Loss: 2.4822, Perplexity: 11.9675\n",
      "Epoch [2/3], Step [38100/41412], Loss: 2.3917, Perplexity: 10.9318\n",
      "Epoch [2/3], Step [38200/41412], Loss: 1.8441, Perplexity: 6.32221\n",
      "Epoch [2/3], Step [38300/41412], Loss: 2.4387, Perplexity: 11.4576\n",
      "Epoch [2/3], Step [38400/41412], Loss: 2.3654, Perplexity: 10.6486\n",
      "Epoch [2/3], Step [38500/41412], Loss: 2.1111, Perplexity: 8.25752\n",
      "Epoch [2/3], Step [38600/41412], Loss: 2.3492, Perplexity: 10.4773\n",
      "Epoch [2/3], Step [38700/41412], Loss: 2.1038, Perplexity: 8.19742\n",
      "Epoch [2/3], Step [38800/41412], Loss: 2.8051, Perplexity: 16.5292\n",
      "Epoch [2/3], Step [38900/41412], Loss: 2.3182, Perplexity: 10.1579\n",
      "Epoch [2/3], Step [39000/41412], Loss: 1.9660, Perplexity: 7.14228\n",
      "Epoch [2/3], Step [39100/41412], Loss: 2.3979, Perplexity: 11.0003\n",
      "Epoch [2/3], Step [39200/41412], Loss: 2.2510, Perplexity: 9.49764\n",
      "Epoch [2/3], Step [39300/41412], Loss: 2.3394, Perplexity: 10.3754\n",
      "Epoch [2/3], Step [39400/41412], Loss: 1.9635, Perplexity: 7.12392\n",
      "Epoch [2/3], Step [39500/41412], Loss: 2.3248, Perplexity: 10.2251\n",
      "Epoch [2/3], Step [39600/41412], Loss: 2.6496, Perplexity: 14.1479\n",
      "Epoch [2/3], Step [39700/41412], Loss: 3.0744, Perplexity: 21.6371\n",
      "Epoch [2/3], Step [39800/41412], Loss: 2.2997, Perplexity: 9.97125\n",
      "Epoch [2/3], Step [39900/41412], Loss: 2.1693, Perplexity: 8.75192\n",
      "Epoch [2/3], Step [40000/41412], Loss: 2.4033, Perplexity: 11.0591\n",
      "Epoch [2/3], Step [40100/41412], Loss: 2.0383, Perplexity: 7.677523\n",
      "Epoch [2/3], Step [40200/41412], Loss: 1.9662, Perplexity: 7.14340\n",
      "Epoch [2/3], Step [40300/41412], Loss: 2.3307, Perplexity: 10.2848\n",
      "Epoch [2/3], Step [40400/41412], Loss: 2.2512, Perplexity: 9.49878\n",
      "Epoch [2/3], Step [40500/41412], Loss: 1.9167, Perplexity: 6.79843\n",
      "Epoch [2/3], Step [40600/41412], Loss: 1.6252, Perplexity: 5.07952\n",
      "Epoch [2/3], Step [40700/41412], Loss: 1.9757, Perplexity: 7.211952\n",
      "Epoch [2/3], Step [40800/41412], Loss: 1.8339, Perplexity: 6.25838\n",
      "Epoch [2/3], Step [40900/41412], Loss: 2.0351, Perplexity: 7.65274\n",
      "Epoch [2/3], Step [41000/41412], Loss: 1.9924, Perplexity: 7.33315\n",
      "Epoch [2/3], Step [41100/41412], Loss: 2.6895, Perplexity: 14.7240\n",
      "Epoch [2/3], Step [41200/41412], Loss: 1.8744, Perplexity: 6.51712\n",
      "Epoch [2/3], Step [41300/41412], Loss: 2.8458, Perplexity: 17.2149\n",
      "Epoch [2/3], Step [41400/41412], Loss: 2.8163, Perplexity: 16.7141\n",
      "Epoch [3/3], Step [100/41412], Loss: 2.1977, Perplexity: 9.0044652\n",
      "Epoch [3/3], Step [200/41412], Loss: 2.2178, Perplexity: 9.18752\n",
      "Epoch [3/3], Step [300/41412], Loss: 2.3851, Perplexity: 10.8598\n",
      "Epoch [3/3], Step [400/41412], Loss: 1.7540, Perplexity: 5.77794\n",
      "Epoch [3/3], Step [500/41412], Loss: 2.3615, Perplexity: 10.6065\n",
      "Epoch [3/3], Step [600/41412], Loss: 1.9854, Perplexity: 7.28163\n",
      "Epoch [3/3], Step [700/41412], Loss: 2.7714, Perplexity: 15.9808\n",
      "Epoch [3/3], Step [800/41412], Loss: 2.2406, Perplexity: 9.39898\n",
      "Epoch [3/3], Step [900/41412], Loss: 2.1168, Perplexity: 8.30459\n",
      "Epoch [3/3], Step [1000/41412], Loss: 2.7419, Perplexity: 15.5167\n",
      "Epoch [3/3], Step [1100/41412], Loss: 2.5047, Perplexity: 12.2402\n",
      "Epoch [3/3], Step [1200/41412], Loss: 2.3175, Perplexity: 10.1508\n",
      "Epoch [3/3], Step [1300/41412], Loss: 2.2321, Perplexity: 9.31937\n",
      "Epoch [3/3], Step [1400/41412], Loss: 2.5207, Perplexity: 12.4371\n",
      "Epoch [3/3], Step [1500/41412], Loss: 2.1587, Perplexity: 8.66016\n",
      "Epoch [3/3], Step [1600/41412], Loss: 2.9799, Perplexity: 19.6854\n",
      "Epoch [3/3], Step [1700/41412], Loss: 2.1782, Perplexity: 8.83084\n",
      "Epoch [3/3], Step [1800/41412], Loss: 2.1550, Perplexity: 8.62751\n",
      "Epoch [3/3], Step [1900/41412], Loss: 2.3183, Perplexity: 10.1585\n",
      "Epoch [3/3], Step [2000/41412], Loss: 2.2287, Perplexity: 9.28817\n",
      "Epoch [3/3], Step [2100/41412], Loss: 2.5528, Perplexity: 12.8434\n",
      "Epoch [3/3], Step [2200/41412], Loss: 2.0164, Perplexity: 7.51112\n",
      "Epoch [3/3], Step [2300/41412], Loss: 2.0567, Perplexity: 7.82011\n",
      "Epoch [3/3], Step [2400/41412], Loss: 2.2067, Perplexity: 9.08561\n",
      "Epoch [3/3], Step [2500/41412], Loss: 2.2010, Perplexity: 9.03449\n",
      "Epoch [3/3], Step [2600/41412], Loss: 2.2996, Perplexity: 9.97005\n",
      "Epoch [3/3], Step [2700/41412], Loss: 2.1746, Perplexity: 8.79887\n",
      "Epoch [3/3], Step [2800/41412], Loss: 3.4626, Perplexity: 31.9002\n",
      "Epoch [3/3], Step [2900/41412], Loss: 2.5269, Perplexity: 12.5143\n",
      "Epoch [3/3], Step [3000/41412], Loss: 2.2136, Perplexity: 9.14860\n",
      "Epoch [3/3], Step [3100/41412], Loss: 1.8413, Perplexity: 6.30488\n",
      "Epoch [3/3], Step [3200/41412], Loss: 2.1081, Perplexity: 8.23308\n",
      "Epoch [3/3], Step [3300/41412], Loss: 2.3393, Perplexity: 10.3740\n",
      "Epoch [3/3], Step [3400/41412], Loss: 2.3069, Perplexity: 10.0429\n",
      "Epoch [3/3], Step [3500/41412], Loss: 2.0117, Perplexity: 7.47624\n",
      "Epoch [3/3], Step [3600/41412], Loss: 2.9549, Perplexity: 19.1991\n",
      "Epoch [3/3], Step [3700/41412], Loss: 2.0590, Perplexity: 7.83786\n",
      "Epoch [3/3], Step [3800/41412], Loss: 1.9863, Perplexity: 7.28849\n",
      "Epoch [3/3], Step [3900/41412], Loss: 2.3600, Perplexity: 10.5913\n",
      "Epoch [3/3], Step [4000/41412], Loss: 2.0397, Perplexity: 7.68855\n",
      "Epoch [3/3], Step [4100/41412], Loss: 1.6996, Perplexity: 5.47207\n",
      "Epoch [3/3], Step [4200/41412], Loss: 1.8858, Perplexity: 6.59133\n",
      "Epoch [3/3], Step [4300/41412], Loss: 2.1950, Perplexity: 8.97963\n",
      "Epoch [3/3], Step [4400/41412], Loss: 2.1280, Perplexity: 8.39848\n",
      "Epoch [3/3], Step [4500/41412], Loss: 2.9517, Perplexity: 19.1393\n",
      "Epoch [3/3], Step [4600/41412], Loss: 1.7347, Perplexity: 5.66752\n",
      "Epoch [3/3], Step [4700/41412], Loss: 1.9292, Perplexity: 6.88416\n",
      "Epoch [3/3], Step [4800/41412], Loss: 2.0187, Perplexity: 7.52857\n",
      "Epoch [3/3], Step [4900/41412], Loss: 2.2240, Perplexity: 9.24435\n",
      "Epoch [3/3], Step [5000/41412], Loss: 2.3803, Perplexity: 10.8078\n",
      "Epoch [3/3], Step [5100/41412], Loss: 2.2883, Perplexity: 9.85828\n",
      "Epoch [3/3], Step [5200/41412], Loss: 2.1276, Perplexity: 8.39445\n",
      "Epoch [3/3], Step [5300/41412], Loss: 1.8376, Perplexity: 6.28155\n",
      "Epoch [3/3], Step [5400/41412], Loss: 2.6700, Perplexity: 14.4394\n",
      "Epoch [3/3], Step [5500/41412], Loss: 2.6415, Perplexity: 14.0337\n",
      "Epoch [3/3], Step [5600/41412], Loss: 2.0479, Perplexity: 7.75189\n",
      "Epoch [3/3], Step [5700/41412], Loss: 1.9320, Perplexity: 6.90344\n",
      "Epoch [3/3], Step [5800/41412], Loss: 2.1871, Perplexity: 8.90941\n",
      "Epoch [3/3], Step [5900/41412], Loss: 1.8896, Perplexity: 6.61699\n",
      "Epoch [3/3], Step [6000/41412], Loss: 2.6395, Perplexity: 14.0057\n",
      "Epoch [3/3], Step [6100/41412], Loss: 1.8966, Perplexity: 6.66342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [6200/41412], Loss: 2.2666, Perplexity: 9.64647\n",
      "Epoch [3/3], Step [6300/41412], Loss: 2.0893, Perplexity: 8.07892\n",
      "Epoch [3/3], Step [6400/41412], Loss: 2.2714, Perplexity: 9.69266\n",
      "Epoch [3/3], Step [6500/41412], Loss: 2.0085, Perplexity: 7.45255\n",
      "Epoch [3/3], Step [6600/41412], Loss: 2.5243, Perplexity: 12.4823\n",
      "Epoch [3/3], Step [6700/41412], Loss: 2.0924, Perplexity: 8.10470\n",
      "Epoch [3/3], Step [6800/41412], Loss: 2.2069, Perplexity: 9.08746\n",
      "Epoch [3/3], Step [6900/41412], Loss: 2.0081, Perplexity: 7.44899\n",
      "Epoch [3/3], Step [7000/41412], Loss: 2.7843, Perplexity: 16.1883\n",
      "Epoch [3/3], Step [7100/41412], Loss: 2.5758, Perplexity: 13.1419\n",
      "Epoch [3/3], Step [7200/41412], Loss: 2.1599, Perplexity: 8.67067\n",
      "Epoch [3/3], Step [7300/41412], Loss: 2.4821, Perplexity: 11.9664\n",
      "Epoch [3/3], Step [7400/41412], Loss: 2.6447, Perplexity: 14.0795\n",
      "Epoch [3/3], Step [7500/41412], Loss: 2.2258, Perplexity: 9.26095\n",
      "Epoch [3/3], Step [7600/41412], Loss: 1.8976, Perplexity: 6.67001\n",
      "Epoch [3/3], Step [7700/41412], Loss: 1.9927, Perplexity: 7.33535\n",
      "Epoch [3/3], Step [7800/41412], Loss: 2.2313, Perplexity: 9.31233\n",
      "Epoch [3/3], Step [7900/41412], Loss: 2.2652, Perplexity: 9.63261\n",
      "Epoch [3/3], Step [8000/41412], Loss: 2.5042, Perplexity: 12.2342\n",
      "Epoch [3/3], Step [8100/41412], Loss: 2.2501, Perplexity: 9.48911\n",
      "Epoch [3/3], Step [8200/41412], Loss: 1.9084, Perplexity: 6.74223\n",
      "Epoch [3/3], Step [8300/41412], Loss: 2.6583, Perplexity: 14.2715\n",
      "Epoch [3/3], Step [8400/41412], Loss: 1.9627, Perplexity: 7.11855\n",
      "Epoch [3/3], Step [8500/41412], Loss: 2.1376, Perplexity: 8.47918\n",
      "Epoch [3/3], Step [8600/41412], Loss: 2.0133, Perplexity: 7.48787\n",
      "Epoch [3/3], Step [8700/41412], Loss: 2.1067, Perplexity: 8.22133\n",
      "Epoch [3/3], Step [8800/41412], Loss: 1.9070, Perplexity: 6.73275\n",
      "Epoch [3/3], Step [8900/41412], Loss: 2.3725, Perplexity: 10.7243\n",
      "Epoch [3/3], Step [9000/41412], Loss: 2.4855, Perplexity: 12.0068\n",
      "Epoch [3/3], Step [9100/41412], Loss: 2.7012, Perplexity: 14.8973\n",
      "Epoch [3/3], Step [9200/41412], Loss: 2.6446, Perplexity: 14.0784\n",
      "Epoch [3/3], Step [9300/41412], Loss: 1.7330, Perplexity: 5.65755\n",
      "Epoch [3/3], Step [9400/41412], Loss: 1.8146, Perplexity: 6.13886\n",
      "Epoch [3/3], Step [9500/41412], Loss: 2.0213, Perplexity: 7.54845\n",
      "Epoch [3/3], Step [9600/41412], Loss: 3.0924, Perplexity: 22.0307\n",
      "Epoch [3/3], Step [9700/41412], Loss: 2.3245, Perplexity: 10.2211\n",
      "Epoch [3/3], Step [9800/41412], Loss: 1.6578, Perplexity: 5.24758\n",
      "Epoch [3/3], Step [9900/41412], Loss: 2.5085, Perplexity: 12.2864\n",
      "Epoch [3/3], Step [10000/41412], Loss: 2.3946, Perplexity: 10.9634\n",
      "Epoch [3/3], Step [10100/41412], Loss: 2.2063, Perplexity: 9.08214\n",
      "Epoch [3/3], Step [10200/41412], Loss: 2.1955, Perplexity: 8.98475\n",
      "Epoch [3/3], Step [10300/41412], Loss: 2.0940, Perplexity: 8.11722\n",
      "Epoch [3/3], Step [10400/41412], Loss: 2.3925, Perplexity: 10.9408\n",
      "Epoch [3/3], Step [10500/41412], Loss: 2.3554, Perplexity: 10.54268\n",
      "Epoch [3/3], Step [10600/41412], Loss: 2.0709, Perplexity: 7.93235\n",
      "Epoch [3/3], Step [10700/41412], Loss: 1.8709, Perplexity: 6.49416\n",
      "Epoch [3/3], Step [10800/41412], Loss: 2.5732, Perplexity: 13.1076\n",
      "Epoch [3/3], Step [10900/41412], Loss: 2.2519, Perplexity: 9.50622\n",
      "Epoch [3/3], Step [11000/41412], Loss: 1.8534, Perplexity: 6.38136\n",
      "Epoch [3/3], Step [11100/41412], Loss: 2.2596, Perplexity: 9.57934\n",
      "Epoch [3/3], Step [11200/41412], Loss: 2.5856, Perplexity: 13.2708\n",
      "Epoch [3/3], Step [11300/41412], Loss: 2.0815, Perplexity: 8.01631\n",
      "Epoch [3/3], Step [11400/41412], Loss: 2.1388, Perplexity: 8.48950\n",
      "Epoch [3/3], Step [11500/41412], Loss: 2.0507, Perplexity: 7.77362\n",
      "Epoch [3/3], Step [11600/41412], Loss: 2.3996, Perplexity: 11.0188\n",
      "Epoch [3/3], Step [11700/41412], Loss: 2.3298, Perplexity: 10.2760\n",
      "Epoch [3/3], Step [11800/41412], Loss: 1.7584, Perplexity: 5.80290\n",
      "Epoch [3/3], Step [11900/41412], Loss: 1.9424, Perplexity: 6.97527\n",
      "Epoch [3/3], Step [12000/41412], Loss: 2.7531, Perplexity: 15.6909\n",
      "Epoch [3/3], Step [12100/41412], Loss: 2.7228, Perplexity: 15.2235\n",
      "Epoch [3/3], Step [12200/41412], Loss: 2.1714, Perplexity: 8.77046\n",
      "Epoch [3/3], Step [12300/41412], Loss: 1.8604, Perplexity: 6.42627\n",
      "Epoch [3/3], Step [12400/41412], Loss: 2.0356, Perplexity: 7.65696\n",
      "Epoch [3/3], Step [12500/41412], Loss: 2.2408, Perplexity: 9.40072\n",
      "Epoch [3/3], Step [12600/41412], Loss: 2.5650, Perplexity: 13.0009\n",
      "Epoch [3/3], Step [12700/41412], Loss: 2.4354, Perplexity: 11.4204\n",
      "Epoch [3/3], Step [12800/41412], Loss: 2.5308, Perplexity: 12.5632\n",
      "Epoch [3/3], Step [12900/41412], Loss: 2.4371, Perplexity: 11.4397\n",
      "Epoch [3/3], Step [13000/41412], Loss: 2.7386, Perplexity: 15.4646\n",
      "Epoch [3/3], Step [13100/41412], Loss: 1.8825, Perplexity: 6.56964\n",
      "Epoch [3/3], Step [13200/41412], Loss: 3.3175, Perplexity: 27.5912\n",
      "Epoch [3/3], Step [13300/41412], Loss: 2.6254, Perplexity: 13.8099\n",
      "Epoch [3/3], Step [13400/41412], Loss: 2.0030, Perplexity: 7.41144\n",
      "Epoch [3/3], Step [13500/41412], Loss: 1.7942, Perplexity: 6.01454\n",
      "Epoch [3/3], Step [13600/41412], Loss: 2.2163, Perplexity: 9.17340\n",
      "Epoch [3/3], Step [13700/41412], Loss: 2.6137, Perplexity: 13.6498\n",
      "Epoch [3/3], Step [13800/41412], Loss: 2.9201, Perplexity: 18.5432\n",
      "Epoch [3/3], Step [13900/41412], Loss: 1.9047, Perplexity: 6.71739\n",
      "Epoch [3/3], Step [14000/41412], Loss: 2.0593, Perplexity: 7.84033\n",
      "Epoch [3/3], Step [14100/41412], Loss: 2.2862, Perplexity: 9.83804\n",
      "Epoch [3/3], Step [14200/41412], Loss: 2.0508, Perplexity: 7.77442\n",
      "Epoch [3/3], Step [14300/41412], Loss: 2.0653, Perplexity: 7.88737\n",
      "Epoch [3/3], Step [14400/41412], Loss: 1.8615, Perplexity: 6.43337\n",
      "Epoch [3/3], Step [14500/41412], Loss: 2.0975, Perplexity: 8.14602\n",
      "Epoch [3/3], Step [14600/41412], Loss: 2.1822, Perplexity: 8.86572\n",
      "Epoch [3/3], Step [14700/41412], Loss: 2.2853, Perplexity: 9.82830\n",
      "Epoch [3/3], Step [14800/41412], Loss: 1.6992, Perplexity: 5.46952\n",
      "Epoch [3/3], Step [14900/41412], Loss: 2.1102, Perplexity: 8.25025\n",
      "Epoch [3/3], Step [15000/41412], Loss: 2.2746, Perplexity: 9.72377\n",
      "Epoch [3/3], Step [15100/41412], Loss: 2.6870, Perplexity: 14.6874\n",
      "Epoch [3/3], Step [15200/41412], Loss: 2.4539, Perplexity: 11.6333\n",
      "Epoch [3/3], Step [15300/41412], Loss: 2.9685, Perplexity: 19.4626\n",
      "Epoch [3/3], Step [15400/41412], Loss: 2.5120, Perplexity: 12.3296\n",
      "Epoch [3/3], Step [15500/41412], Loss: 3.0432, Perplexity: 20.9729\n",
      "Epoch [3/3], Step [15600/41412], Loss: 1.6678, Perplexity: 5.30057\n",
      "Epoch [3/3], Step [15700/41412], Loss: 2.4344, Perplexity: 11.4094\n",
      "Epoch [3/3], Step [15800/41412], Loss: 1.8064, Perplexity: 6.08829\n",
      "Epoch [3/3], Step [15900/41412], Loss: 2.6732, Perplexity: 14.4859\n",
      "Epoch [3/3], Step [16000/41412], Loss: 2.6630, Perplexity: 14.3388\n",
      "Epoch [3/3], Step [16100/41412], Loss: 2.5809, Perplexity: 13.2084\n",
      "Epoch [3/3], Step [16200/41412], Loss: 2.1354, Perplexity: 8.46046\n",
      "Epoch [3/3], Step [16300/41412], Loss: 2.0324, Perplexity: 7.63248\n",
      "Epoch [3/3], Step [16400/41412], Loss: 2.0084, Perplexity: 7.45127\n",
      "Epoch [3/3], Step [16500/41412], Loss: 1.8682, Perplexity: 6.47638\n",
      "Epoch [3/3], Step [16600/41412], Loss: 2.0385, Perplexity: 7.67941\n",
      "Epoch [3/3], Step [16700/41412], Loss: 2.3001, Perplexity: 9.97483\n",
      "Epoch [3/3], Step [16800/41412], Loss: 2.6628, Perplexity: 14.3364\n",
      "Epoch [3/3], Step [16900/41412], Loss: 2.0314, Perplexity: 7.62483\n",
      "Epoch [3/3], Step [17000/41412], Loss: 2.0362, Perplexity: 7.66130\n",
      "Epoch [3/3], Step [17100/41412], Loss: 2.2363, Perplexity: 9.35917\n",
      "Epoch [3/3], Step [17200/41412], Loss: 1.8600, Perplexity: 6.42343\n",
      "Epoch [3/3], Step [17300/41412], Loss: 2.3609, Perplexity: 10.6001\n",
      "Epoch [3/3], Step [17400/41412], Loss: 1.5537, Perplexity: 4.72912\n",
      "Epoch [3/3], Step [17500/41412], Loss: 1.9462, Perplexity: 7.00200\n",
      "Epoch [3/3], Step [17600/41412], Loss: 2.0960, Perplexity: 8.13359\n",
      "Epoch [3/3], Step [17700/41412], Loss: 2.3221, Perplexity: 10.1967\n",
      "Epoch [3/3], Step [17800/41412], Loss: 2.4116, Perplexity: 11.1517\n",
      "Epoch [3/3], Step [17900/41412], Loss: 1.9052, Perplexity: 6.72102\n",
      "Epoch [3/3], Step [18000/41412], Loss: 1.7483, Perplexity: 5.74472\n",
      "Epoch [3/3], Step [18100/41412], Loss: 2.5590, Perplexity: 12.9234\n",
      "Epoch [3/3], Step [18200/41412], Loss: 1.8075, Perplexity: 6.09557\n",
      "Epoch [3/3], Step [18300/41412], Loss: 2.2924, Perplexity: 9.89820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [18400/41412], Loss: 2.6378, Perplexity: 13.9818\n",
      "Epoch [3/3], Step [18500/41412], Loss: 1.6457, Perplexity: 5.18460\n",
      "Epoch [3/3], Step [18600/41412], Loss: 2.7285, Perplexity: 15.3104\n",
      "Epoch [3/3], Step [18700/41412], Loss: 2.3590, Perplexity: 10.5801\n",
      "Epoch [3/3], Step [18800/41412], Loss: 2.9451, Perplexity: 19.0127\n",
      "Epoch [3/3], Step [18900/41412], Loss: 1.9010, Perplexity: 6.69272\n",
      "Epoch [3/3], Step [19000/41412], Loss: 2.1076, Perplexity: 8.22859\n",
      "Epoch [3/3], Step [19100/41412], Loss: 2.9943, Perplexity: 19.9707\n",
      "Epoch [3/3], Step [19200/41412], Loss: 2.3419, Perplexity: 10.4013\n",
      "Epoch [3/3], Step [19300/41412], Loss: 2.8684, Perplexity: 17.6087\n",
      "Epoch [3/3], Step [19400/41412], Loss: 2.4888, Perplexity: 12.0463\n",
      "Epoch [3/3], Step [19500/41412], Loss: 2.0558, Perplexity: 7.81284\n",
      "Epoch [3/3], Step [19600/41412], Loss: 2.0246, Perplexity: 7.57291\n",
      "Epoch [3/3], Step [19700/41412], Loss: 2.0000, Perplexity: 7.38912\n",
      "Epoch [3/3], Step [19800/41412], Loss: 2.3213, Perplexity: 10.1889\n",
      "Epoch [3/3], Step [19900/41412], Loss: 2.1390, Perplexity: 8.49098\n",
      "Epoch [3/3], Step [20000/41412], Loss: 2.8457, Perplexity: 17.2144\n",
      "Epoch [3/3], Step [20100/41412], Loss: 2.2876, Perplexity: 9.85173\n",
      "Epoch [3/3], Step [20200/41412], Loss: 1.8434, Perplexity: 6.31781\n",
      "Epoch [3/3], Step [20300/41412], Loss: 2.0325, Perplexity: 7.63293\n",
      "Epoch [3/3], Step [20400/41412], Loss: 2.3559, Perplexity: 10.5473\n",
      "Epoch [3/3], Step [20500/41412], Loss: 2.3466, Perplexity: 10.4497\n",
      "Epoch [3/3], Step [20600/41412], Loss: 3.2646, Perplexity: 26.1700\n",
      "Epoch [3/3], Step [20700/41412], Loss: 2.4318, Perplexity: 11.3791\n",
      "Epoch [3/3], Step [20800/41412], Loss: 1.7708, Perplexity: 5.87531\n",
      "Epoch [3/3], Step [20900/41412], Loss: 2.2469, Perplexity: 9.45884\n",
      "Epoch [3/3], Step [21000/41412], Loss: 1.6792, Perplexity: 5.36131\n",
      "Epoch [3/3], Step [21100/41412], Loss: 2.6071, Perplexity: 13.5593\n",
      "Epoch [3/3], Step [21200/41412], Loss: 1.9202, Perplexity: 6.82232\n",
      "Epoch [3/3], Step [21300/41412], Loss: 1.9484, Perplexity: 7.01789\n",
      "Epoch [3/3], Step [21400/41412], Loss: 2.7008, Perplexity: 14.8912\n",
      "Epoch [3/3], Step [21500/41412], Loss: 2.2518, Perplexity: 9.50504\n",
      "Epoch [3/3], Step [21600/41412], Loss: 2.5402, Perplexity: 12.6824\n",
      "Epoch [3/3], Step [21700/41412], Loss: 1.8259, Perplexity: 6.20841\n",
      "Epoch [3/3], Step [21800/41412], Loss: 1.9677, Perplexity: 7.15426\n",
      "Epoch [3/3], Step [21900/41412], Loss: 2.0158, Perplexity: 7.50655\n",
      "Epoch [3/3], Step [22000/41412], Loss: 2.3551, Perplexity: 10.5396\n",
      "Epoch [3/3], Step [22100/41412], Loss: 2.3661, Perplexity: 10.6561\n",
      "Epoch [3/3], Step [22200/41412], Loss: 2.2157, Perplexity: 9.16800\n",
      "Epoch [3/3], Step [22300/41412], Loss: 2.2516, Perplexity: 9.50275\n",
      "Epoch [3/3], Step [22400/41412], Loss: 2.2276, Perplexity: 9.27743\n",
      "Epoch [3/3], Step [22500/41412], Loss: 2.0294, Perplexity: 7.60980\n",
      "Epoch [3/3], Step [22600/41412], Loss: 2.1191, Perplexity: 8.32398\n",
      "Epoch [3/3], Step [22700/41412], Loss: 2.2051, Perplexity: 9.07148\n",
      "Epoch [3/3], Step [22800/41412], Loss: 1.8926, Perplexity: 6.63657\n",
      "Epoch [3/3], Step [22900/41412], Loss: 2.1800, Perplexity: 8.84593\n",
      "Epoch [3/3], Step [23000/41412], Loss: 1.9642, Perplexity: 7.12907\n",
      "Epoch [3/3], Step [23100/41412], Loss: 1.8520, Perplexity: 6.37253\n",
      "Epoch [3/3], Step [23200/41412], Loss: 2.0322, Perplexity: 7.63086\n",
      "Epoch [3/3], Step [23300/41412], Loss: 2.0907, Perplexity: 8.09080\n",
      "Epoch [3/3], Step [23400/41412], Loss: 1.6258, Perplexity: 5.08263\n",
      "Epoch [3/3], Step [23500/41412], Loss: 2.6574, Perplexity: 14.2587\n",
      "Epoch [3/3], Step [23600/41412], Loss: 2.3631, Perplexity: 10.6237\n",
      "Epoch [3/3], Step [23700/41412], Loss: 1.8915, Perplexity: 6.62910\n",
      "Epoch [3/3], Step [23800/41412], Loss: 2.2747, Perplexity: 9.72503\n",
      "Epoch [3/3], Step [23900/41412], Loss: 2.5856, Perplexity: 13.2716\n",
      "Epoch [3/3], Step [24000/41412], Loss: 2.4471, Perplexity: 11.5547\n",
      "Epoch [3/3], Step [24100/41412], Loss: 2.3853, Perplexity: 10.8619\n",
      "Epoch [3/3], Step [24200/41412], Loss: 2.2262, Perplexity: 9.26412\n",
      "Epoch [3/3], Step [24300/41412], Loss: 2.2503, Perplexity: 9.49054\n",
      "Epoch [3/3], Step [24400/41412], Loss: 2.1091, Perplexity: 8.24110\n",
      "Epoch [3/3], Step [24500/41412], Loss: 2.1010, Perplexity: 8.17452\n",
      "Epoch [3/3], Step [24600/41412], Loss: 2.4395, Perplexity: 11.4679\n",
      "Epoch [3/3], Step [24700/41412], Loss: 2.4310, Perplexity: 11.3706\n",
      "Epoch [3/3], Step [24800/41412], Loss: 2.1000, Perplexity: 8.16610\n",
      "Epoch [3/3], Step [24900/41412], Loss: 2.2122, Perplexity: 9.13582\n",
      "Epoch [3/3], Step [25000/41412], Loss: 1.9917, Perplexity: 7.32819\n",
      "Epoch [3/3], Step [25100/41412], Loss: 2.1738, Perplexity: 8.79148\n",
      "Epoch [3/3], Step [25200/41412], Loss: 2.3548, Perplexity: 10.5358\n",
      "Epoch [3/3], Step [25300/41412], Loss: 2.5975, Perplexity: 13.4296\n",
      "Epoch [3/3], Step [25400/41412], Loss: 2.1064, Perplexity: 8.21877\n",
      "Epoch [3/3], Step [25500/41412], Loss: 1.9153, Perplexity: 6.78908\n",
      "Epoch [3/3], Step [25600/41412], Loss: 2.2004, Perplexity: 9.02887\n",
      "Epoch [3/3], Step [25700/41412], Loss: 2.0875, Perplexity: 8.06503\n",
      "Epoch [3/3], Step [25800/41412], Loss: 2.1138, Perplexity: 8.27994\n",
      "Epoch [3/3], Step [25900/41412], Loss: 2.6647, Perplexity: 14.3642\n",
      "Epoch [3/3], Step [26000/41412], Loss: 2.1929, Perplexity: 8.96073\n",
      "Epoch [3/3], Step [26100/41412], Loss: 2.4344, Perplexity: 11.4095\n",
      "Epoch [3/3], Step [26200/41412], Loss: 1.8990, Perplexity: 6.67911\n",
      "Epoch [3/3], Step [26300/41412], Loss: 2.1729, Perplexity: 8.78408\n",
      "Epoch [3/3], Step [26400/41412], Loss: 2.0465, Perplexity: 7.74071\n",
      "Epoch [3/3], Step [26500/41412], Loss: 2.1421, Perplexity: 8.51691\n",
      "Epoch [3/3], Step [26600/41412], Loss: 2.2329, Perplexity: 9.32651\n",
      "Epoch [3/3], Step [26700/41412], Loss: 2.9642, Perplexity: 19.3787\n",
      "Epoch [3/3], Step [26800/41412], Loss: 2.3665, Perplexity: 10.6599\n",
      "Epoch [3/3], Step [26900/41412], Loss: 2.1177, Perplexity: 8.31246\n",
      "Epoch [3/3], Step [27000/41412], Loss: 2.2021, Perplexity: 9.04421\n",
      "Epoch [3/3], Step [27100/41412], Loss: 2.1255, Perplexity: 8.37759\n",
      "Epoch [3/3], Step [27200/41412], Loss: 1.8874, Perplexity: 6.60246\n",
      "Epoch [3/3], Step [27300/41412], Loss: 2.6948, Perplexity: 14.8022\n",
      "Epoch [3/3], Step [27400/41412], Loss: 2.2108, Perplexity: 9.12342\n",
      "Epoch [3/3], Step [27500/41412], Loss: 2.3701, Perplexity: 10.6981\n",
      "Epoch [3/3], Step [27600/41412], Loss: 1.8482, Perplexity: 6.34872\n",
      "Epoch [3/3], Step [27700/41412], Loss: 2.3572, Perplexity: 10.5616\n",
      "Epoch [3/3], Step [27800/41412], Loss: 2.7774, Perplexity: 16.0773\n",
      "Epoch [3/3], Step [27900/41412], Loss: 2.6379, Perplexity: 13.9837\n",
      "Epoch [3/3], Step [28000/41412], Loss: 2.6593, Perplexity: 14.2869\n",
      "Epoch [3/3], Step [28100/41412], Loss: 2.0725, Perplexity: 7.94462\n",
      "Epoch [3/3], Step [28200/41412], Loss: 2.0870, Perplexity: 8.060360\n",
      "Epoch [3/3], Step [28300/41412], Loss: 2.6056, Perplexity: 13.5396\n",
      "Epoch [3/3], Step [28400/41412], Loss: 1.8641, Perplexity: 6.45020\n",
      "Epoch [3/3], Step [28500/41412], Loss: 2.0264, Perplexity: 7.58663\n",
      "Epoch [3/3], Step [28600/41412], Loss: 1.9613, Perplexity: 7.10881\n",
      "Epoch [3/3], Step [28700/41412], Loss: 2.8922, Perplexity: 18.0336\n",
      "Epoch [3/3], Step [28800/41412], Loss: 1.8484, Perplexity: 6.34955\n",
      "Epoch [3/3], Step [28900/41412], Loss: 1.9358, Perplexity: 6.92998\n",
      "Epoch [3/3], Step [29000/41412], Loss: 1.9249, Perplexity: 6.85488\n",
      "Epoch [3/3], Step [29100/41412], Loss: 2.2120, Perplexity: 9.13402\n",
      "Epoch [3/3], Step [29200/41412], Loss: 2.0254, Perplexity: 7.57909\n",
      "Epoch [3/3], Step [29300/41412], Loss: 2.7185, Perplexity: 15.1581\n",
      "Epoch [3/3], Step [29400/41412], Loss: 2.6580, Perplexity: 14.2679\n",
      "Epoch [3/3], Step [29500/41412], Loss: 2.1723, Perplexity: 8.77808\n",
      "Epoch [3/3], Step [29600/41412], Loss: 1.7837, Perplexity: 5.95179\n",
      "Epoch [3/3], Step [29700/41412], Loss: 2.3785, Perplexity: 10.7886\n",
      "Epoch [3/3], Step [29800/41412], Loss: 2.0296, Perplexity: 7.61092\n",
      "Epoch [3/3], Step [29900/41412], Loss: 1.8514, Perplexity: 6.36879\n",
      "Epoch [3/3], Step [30000/41412], Loss: 1.6707, Perplexity: 5.31614\n",
      "Epoch [3/3], Step [30100/41412], Loss: 2.0909, Perplexity: 8.09255\n",
      "Epoch [3/3], Step [30200/41412], Loss: 2.4412, Perplexity: 11.4863\n",
      "Epoch [3/3], Step [30300/41412], Loss: 2.5239, Perplexity: 12.4772\n",
      "Epoch [3/3], Step [30400/41412], Loss: 1.7963, Perplexity: 6.02742\n",
      "Epoch [3/3], Step [30500/41412], Loss: 2.5916, Perplexity: 13.3506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [30600/41412], Loss: 2.0966, Perplexity: 8.13814\n",
      "Epoch [3/3], Step [30700/41412], Loss: 1.8874, Perplexity: 6.60198\n",
      "Epoch [3/3], Step [30800/41412], Loss: 2.0875, Perplexity: 8.06518\n",
      "Epoch [3/3], Step [30900/41412], Loss: 1.7863, Perplexity: 5.96719\n",
      "Epoch [3/3], Step [31000/41412], Loss: 1.9493, Perplexity: 7.02374\n",
      "Epoch [3/3], Step [31100/41412], Loss: 2.3743, Perplexity: 10.7434\n",
      "Epoch [3/3], Step [31200/41412], Loss: 2.0026, Perplexity: 7.40849\n",
      "Epoch [3/3], Step [31300/41412], Loss: 2.1069, Perplexity: 8.22250\n",
      "Epoch [3/3], Step [31400/41412], Loss: 2.3899, Perplexity: 10.9129\n",
      "Epoch [3/3], Step [31500/41412], Loss: 2.4160, Perplexity: 11.2015\n",
      "Epoch [3/3], Step [31600/41412], Loss: 2.1029, Perplexity: 8.19000\n",
      "Epoch [3/3], Step [31700/41412], Loss: 2.2315, Perplexity: 9.31354\n",
      "Epoch [3/3], Step [31800/41412], Loss: 1.9681, Perplexity: 7.15724\n",
      "Epoch [3/3], Step [31900/41412], Loss: 2.8497, Perplexity: 17.2823\n",
      "Epoch [3/3], Step [32000/41412], Loss: 2.3242, Perplexity: 10.2188\n",
      "Epoch [3/3], Step [32100/41412], Loss: 1.8134, Perplexity: 6.13156\n",
      "Epoch [3/3], Step [32200/41412], Loss: 2.9916, Perplexity: 19.9174\n",
      "Epoch [3/3], Step [32300/41412], Loss: 2.5923, Perplexity: 13.3599\n",
      "Epoch [3/3], Step [32400/41412], Loss: 2.1397, Perplexity: 8.49663\n",
      "Epoch [3/3], Step [32500/41412], Loss: 2.3235, Perplexity: 10.2112\n",
      "Epoch [3/3], Step [32600/41412], Loss: 2.3850, Perplexity: 10.8594\n",
      "Epoch [3/3], Step [32700/41412], Loss: 3.0877, Perplexity: 21.9261\n",
      "Epoch [3/3], Step [32800/41412], Loss: 2.0535, Perplexity: 7.79545\n",
      "Epoch [3/3], Step [32900/41412], Loss: 2.4955, Perplexity: 12.1273\n",
      "Epoch [3/3], Step [33000/41412], Loss: 2.2136, Perplexity: 9.14899\n",
      "Epoch [3/3], Step [33100/41412], Loss: 2.6675, Perplexity: 14.4043\n",
      "Epoch [3/3], Step [33200/41412], Loss: 2.0253, Perplexity: 7.57828\n",
      "Epoch [3/3], Step [33300/41412], Loss: 2.0838, Perplexity: 8.03513\n",
      "Epoch [3/3], Step [33400/41412], Loss: 2.1613, Perplexity: 8.68260\n",
      "Epoch [3/3], Step [33500/41412], Loss: 1.4718, Perplexity: 4.35710\n",
      "Epoch [3/3], Step [33600/41412], Loss: 2.1879, Perplexity: 8.91678\n",
      "Epoch [3/3], Step [33700/41412], Loss: 2.0317, Perplexity: 7.62727\n",
      "Epoch [3/3], Step [33800/41412], Loss: 2.3015, Perplexity: 9.98950\n",
      "Epoch [3/3], Step [33900/41412], Loss: 2.2822, Perplexity: 9.79847\n",
      "Epoch [3/3], Step [34000/41412], Loss: 1.8407, Perplexity: 6.30127\n",
      "Epoch [3/3], Step [34100/41412], Loss: 2.0836, Perplexity: 8.03364\n",
      "Epoch [3/3], Step [34200/41412], Loss: 1.7316, Perplexity: 5.64995\n",
      "Epoch [3/3], Step [34300/41412], Loss: 1.9986, Perplexity: 7.37918\n",
      "Epoch [3/3], Step [34400/41412], Loss: 2.1381, Perplexity: 8.48311\n",
      "Epoch [3/3], Step [34500/41412], Loss: 1.9837, Perplexity: 7.26975\n",
      "Epoch [3/3], Step [34600/41412], Loss: 2.0749, Perplexity: 7.96382\n",
      "Epoch [3/3], Step [34700/41412], Loss: 2.0857, Perplexity: 8.05031\n",
      "Epoch [3/3], Step [34800/41412], Loss: 1.9652, Perplexity: 7.13618\n",
      "Epoch [3/3], Step [34900/41412], Loss: 1.9223, Perplexity: 6.83703\n",
      "Epoch [3/3], Step [35000/41412], Loss: 2.0459, Perplexity: 7.73633\n",
      "Epoch [3/3], Step [35100/41412], Loss: 1.8946, Perplexity: 6.64973\n",
      "Epoch [3/3], Step [35200/41412], Loss: 2.2486, Perplexity: 9.47478\n",
      "Epoch [3/3], Step [35300/41412], Loss: 2.1060, Perplexity: 8.21525\n",
      "Epoch [3/3], Step [35400/41412], Loss: 2.2496, Perplexity: 9.48375\n",
      "Epoch [3/3], Step [35500/41412], Loss: 2.9031, Perplexity: 18.2300\n",
      "Epoch [3/3], Step [35600/41412], Loss: 2.4133, Perplexity: 11.1713\n",
      "Epoch [3/3], Step [35700/41412], Loss: 2.3243, Perplexity: 10.2197\n",
      "Epoch [3/3], Step [35800/41412], Loss: 2.3961, Perplexity: 10.9801\n",
      "Epoch [3/3], Step [35900/41412], Loss: 2.3947, Perplexity: 10.9648\n",
      "Epoch [3/3], Step [36000/41412], Loss: 1.8680, Perplexity: 6.47550\n",
      "Epoch [3/3], Step [36100/41412], Loss: 1.8745, Perplexity: 6.51731\n",
      "Epoch [3/3], Step [36200/41412], Loss: 2.5844, Perplexity: 13.2553\n",
      "Epoch [3/3], Step [36300/41412], Loss: 1.8202, Perplexity: 6.17337\n",
      "Epoch [3/3], Step [36400/41412], Loss: 2.7953, Perplexity: 16.3683\n",
      "Epoch [3/3], Step [36500/41412], Loss: 2.3971, Perplexity: 10.9908\n",
      "Epoch [3/3], Step [36600/41412], Loss: 3.0095, Perplexity: 20.2779\n",
      "Epoch [3/3], Step [36700/41412], Loss: 1.8508, Perplexity: 6.36493\n",
      "Epoch [3/3], Step [36800/41412], Loss: 2.4971, Perplexity: 12.1476\n",
      "Epoch [3/3], Step [36900/41412], Loss: 2.2274, Perplexity: 9.27537\n",
      "Epoch [3/3], Step [37000/41412], Loss: 2.4548, Perplexity: 11.6439\n",
      "Epoch [3/3], Step [37100/41412], Loss: 2.1729, Perplexity: 8.78411\n",
      "Epoch [3/3], Step [37200/41412], Loss: 2.0589, Perplexity: 7.83750\n",
      "Epoch [3/3], Step [37300/41412], Loss: 2.6524, Perplexity: 14.1886\n",
      "Epoch [3/3], Step [37400/41412], Loss: 2.5312, Perplexity: 12.5689\n",
      "Epoch [3/3], Step [37500/41412], Loss: 2.4391, Perplexity: 11.4627\n",
      "Epoch [3/3], Step [37600/41412], Loss: 2.3370, Perplexity: 10.3499\n",
      "Epoch [3/3], Step [37700/41412], Loss: 1.5070, Perplexity: 4.51334\n",
      "Epoch [3/3], Step [37800/41412], Loss: 2.0367, Perplexity: 7.66557\n",
      "Epoch [3/3], Step [37900/41412], Loss: 1.9885, Perplexity: 7.30467\n",
      "Epoch [3/3], Step [38000/41412], Loss: 2.1507, Perplexity: 8.59042\n",
      "Epoch [3/3], Step [38100/41412], Loss: 1.8262, Perplexity: 6.21021\n",
      "Epoch [3/3], Step [38200/41412], Loss: 2.0466, Perplexity: 7.74170\n",
      "Epoch [3/3], Step [38300/41412], Loss: 2.4016, Perplexity: 11.0407\n",
      "Epoch [3/3], Step [38400/41412], Loss: 3.1788, Perplexity: 24.0178\n",
      "Epoch [3/3], Step [38500/41412], Loss: 2.6432, Perplexity: 14.0586\n",
      "Epoch [3/3], Step [38600/41412], Loss: 1.9257, Perplexity: 6.86016\n",
      "Epoch [3/3], Step [38700/41412], Loss: 1.6522, Perplexity: 5.21865\n",
      "Epoch [3/3], Step [38800/41412], Loss: 2.3350, Perplexity: 10.3293\n",
      "Epoch [3/3], Step [38900/41412], Loss: 2.2878, Perplexity: 9.85283\n",
      "Epoch [3/3], Step [39000/41412], Loss: 1.9845, Perplexity: 7.27565\n",
      "Epoch [3/3], Step [39100/41412], Loss: 2.0551, Perplexity: 7.80762\n",
      "Epoch [3/3], Step [39200/41412], Loss: 2.1439, Perplexity: 8.53283\n",
      "Epoch [3/3], Step [39300/41412], Loss: 1.9739, Perplexity: 7.19881\n",
      "Epoch [3/3], Step [39400/41412], Loss: 2.2895, Perplexity: 9.86982\n",
      "Epoch [3/3], Step [39500/41412], Loss: 2.0347, Perplexity: 7.65022\n",
      "Epoch [3/3], Step [39581/41412], Loss: 2.3937, Perplexity: 10.9543"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "old_time = time.time()\n",
    "# response = requests.request(\"GET\", \n",
    "#                             \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "#                             headers={\"Metadata-Flavor\":\"Google\"})\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "#         if time.time() - old_time > 60:\n",
    "#             old_time = time.time()\n",
    "#             requests.request(\"POST\", \n",
    "#                              \"https://nebula.udacity.com/api/v1/remote/keep-alive\", \n",
    "#                              headers={'Authorization': \"STAR \" + response.text})\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "\n",
    "        # Obtain the batch.\n",
    "        for batch in data_loader:\n",
    "            images, captions = batch[0], batch[1]\n",
    "            break \n",
    "        \n",
    "        # Convert batch of images and captions to Pytorch Variable.\n",
    "        images = to_var(images, volatile=True)\n",
    "        captions = to_var(captions)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.data[0], np.exp(loss.data[0]))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('models', 'decoder-v4-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('models', 'encoder-v4-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: (Optional) Validate your Model\n",
    "\n",
    "To assess potential overfitting, one approach is to assess performance on a validation set.  If you decide to do this **optional** task, you are required to first complete all of the steps in the next notebook in the sequence (**3_Inference.ipynb**); as part of that notebook, you will write and test code (specifically, the `sample` method in the `DecoderRNN` class) that uses your RNN decoder to generate captions.  That code will prove incredibly useful here. \n",
    "\n",
    "If you decide to validate your model, please do not edit the data loader in **data_loader.py**.  Instead, create a new file named **data_loader_val.py** containing the code for obtaining the data loader for the validation data.  You can access:\n",
    "- the validation images at filepath `'/opt/cocoapi/images/train2014/'`, and\n",
    "- the validation image caption annotation file at filepath `'/opt/cocoapi/annotations/captions_val2014.json'`.\n",
    "\n",
    "The suggested approach to validating your model involves creating a json file such as [this one](https://github.com/cocodataset/cocoapi/blob/master/results/captions_val2014_fakecap_results.json) containing your model's predicted captions for the validation images.  Then, you can write your own script or use one that you [find online](https://github.com/tylin/coco-caption) to calculate the BLEU score of your model.  You can read more about the BLEU score, along with other evaluation metrics (such as TEOR and Cider) in section 4.1 of [this paper](https://arxiv.org/pdf/1411.4555.pdf).  For more information about how to use the annotation file, check out the [website](http://cocodataset.org/#download) for the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (Optional) TODO: Validate your model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
